{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1575341988041,"user_tz":300,"elapsed":1252,"user":{"displayName":"Daniel Parangi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64","userId":"01211561534677666164"}},"id":"NuXoAYXf8cs9","outputId":"61f1f1e3-7754-49fb-adc3-86e0aaaf1f83","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/My Drive/College/F19/CS 4740/NLP_P4"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n","/gdrive/My Drive/College/F19/CS 4740/NLP_P4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FSDSEDUXPejS","colab":{}},"source":["# %pip install adabound"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"el697TJ_9TF0","colab":{}},"source":["import pandas as pd\n","import ast\n","from collections import Counter\n","import math\n","import re\n","import csv\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","from torch.nn import init\n","import torch.optim as optim\n","import math\n","import random\n","import os\n","import time\n","from tqdm import tqdm\n","from gensim.models import Word2Vec\n","from torchsummary import summary\n","# import adabound"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LC5r2GaX-bw9","colab":{}},"source":["import nltk.classify.util\n","from nltk.classify import NaiveBayesClassifier\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk import ngrams"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1575341989191,"user_tz":300,"elapsed":2285,"user":{"displayName":"Daniel Parangi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64","userId":"01211561534677666164"}},"id":"TZBYIxzO9UD3","outputId":"de1f58d4-851b-4216-b245-818e82d9f62d","colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["EMBED_DIM = 64 # can be 16, 24, 32, 64, 128\n","HIDDEN_DIM = 32 \n","w2v = 'word2vec' + str(EMBED_DIM) + '.model'\n","WORD2VEC = Word2Vec.load(w2v)\n","NAME = 'NewShoes'\n","# Full: GRU, No Dropout, EMBED_DIM = 32, HIDDEN_DIM = 16, LAYERS = 1, RMSProp, LR=3e-4, +All Features\n","# Baseline: GRU, No Dropout, EMBED_DIM = 32, HIDDEN_DIM = 16, LAYERS = 1, RMSProp, LR=3e-4, +Lengths\n","\n","LAYERS = 4\n","EPOCHS = 1000\n","LR = 3e-4\n","DEVICE = torch.device(\"cuda:0\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9CZmqdQR8meN","colab":{}},"source":["CURRENT = os.curdir\n","MODELS = os.path.join(CURRENT, 'experimental_models')\n","PATH = os.path.join(MODELS, NAME)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vdU2TJqu9ULp","colab":{}},"source":["def embed(train_data, dev_data, test_data):\n","    train_data = train_data.to_numpy()\n","    dev_data = dev_data.to_numpy()\n","    test_data = test_data.to_numpy()\n","\n","    training_data = []\n","    training_data_prime = [[],[]]\n","    train_omega = []\n","    for row1 in train_data:\n","        sample1 = [[],[]]\n","        pos_lst = []\n","        neg_lst = []\n","        pvec_lst = []\n","        nvec_lst = []\n","        for sentence1 in range(1, 5):\n","            lst = [WORD2VEC.wv[word1] for word1 in row1[sentence1].split()]\n","            if sentence1 < 4:\n","                lst1 = lst\n","                lst2 = [WORD2VEC.wv[word1] for word1 in row1[sentence1+1].split()]\n","                lst1 = np.stack(lst1, axis=0)\n","                lst2 = np.stack(lst2, axis=0)\n","                lst1 = np.expand_dims(lst1, axis=0)\n","                lst2 = np.expand_dims(lst2, axis=0)\n","                train_omega.append(((torch.from_numpy(lst1).to(DEVICE), torch.from_numpy(lst2).to(DEVICE)), 1))\n","            sample1[0] += lst\n","            sample1[1] += lst\n","            pos_lst.append(row1[sentence1].split())\n","            neg_lst.append(row1[sentence1].split())\n","            pvec_lst.append([WORD2VEC.wv[word1] for word1 in row1[sentence1].split()])\n","            nvec_lst.append([WORD2VEC.wv[word1] for word1 in row1[sentence1].split()])\n","        if row1[7] == 1:\n","            sample1[0] += [WORD2VEC.wv[word1] for word1 in row1[5].split()]\n","            sample1[1] += [WORD2VEC.wv[word1] for word1 in row1[6].split()]\n","            pos_lst.append(row1[5].split())\n","            neg_lst.append(row1[6].split())\n","            pos_last = [WORD2VEC.wv[word1] for word1 in row1[5].split()]\n","            neg_last = [WORD2VEC.wv[word1] for word1 in row1[6].split()]\n","            pvec_lst.append([WORD2VEC.wv[word1] for word1 in row1[5].split()])\n","            nvec_lst.append([WORD2VEC.wv[word1] for word1 in row1[6].split()])\n","            pos1 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word1] for word1 in row1[4].split()], axis=0), axis=0)).to(DEVICE)\n","            pos2 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word1] for word1 in row1[5].split()], axis=0), axis=0)).to(DEVICE)\n","            neg1 = pos1\n","            neg2 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word1] for word1 in row1[6].split()], axis=0), axis=0)).to(DEVICE)\n","            train_omega.append(((pos1, pos2), 1))\n","            train_omega.append(((neg1, neg2), 0))\n","        elif row1[7] == 2:\n","            sample1[0] += [WORD2VEC.wv[word1] for word1 in row1[6].split()]\n","            sample1[1] += [WORD2VEC.wv[word1] for word1 in row1[5].split()]\n","            pos_lst.append(row1[6].split())\n","            neg_lst.append(row1[5].split())\n","            pos_last = [WORD2VEC.wv[word1] for word1 in row1[6].split()]\n","            neg_last = [WORD2VEC.wv[word1] for word1 in row1[5].split()]\n","            pvec_lst.append([WORD2VEC.wv[word1] for word1 in row1[6].split()])\n","            nvec_lst.append([WORD2VEC.wv[word1] for word1 in row1[5].split()])\n","            pos1 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word1] for word1 in row1[4].split()], axis=0), axis=0)).to(DEVICE)\n","            pos2 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word1] for word1 in row1[6].split()], axis=0), axis=0)).to(DEVICE)\n","            neg1 = pos1\n","            neg2 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word1] for word1 in row1[5].split()], axis=0), axis=0)).to(DEVICE)\n","            train_omega.append(((pos1, pos2), 1))\n","            train_omega.append(((neg1, neg2), 0))\n","        pos = np.stack(sample1[0], axis=0)\n","        neg = np.stack(sample1[1], axis=0)\n","        pos = np.expand_dims(pos, axis=0)\n","        neg = np.expand_dims(neg, axis=0)\n","        pos_last = np.stack(pos_last, axis=0)\n","        pos_last = np.expand_dims(pos_last, axis=0)\n","        neg_last = np.stack(neg_last, axis=0)\n","        neg_last = np.expand_dims(neg_last, axis=0)\n","        train1 = (torch.from_numpy(pos).to(DEVICE), pos_lst, torch.from_numpy(pos_last).to(DEVICE), pvec_lst)\n","        train2 = (torch.from_numpy(neg).to(DEVICE), neg_lst, torch.from_numpy(neg_last).to(DEVICE), nvec_lst)\n","        # training_data.append((train1, train2))\n","        training_data.append((train1, 1))\n","        training_data.append((train2, 0))\n","        training_data_prime[0].append((torch.from_numpy(pos).to(DEVICE), pos_lst))\n","        training_data_prime[1].append((torch.from_numpy(neg).to(DEVICE), neg_lst))\n","\n","    development_data = []\n","    development_data_prime = [[],[]]\n","    dev_omega = []\n","    for row2 in dev_data:\n","        sample2 = [[],[]]\n","        v1_lst = []\n","        v2_lst = []\n","        vec_lst1 = []\n","        vec_lst2 = []\n","        for sentence2 in range(1, 5):\n","            lst = [WORD2VEC.wv[word2] for word2 in row2[sentence2].split()]\n","            sample2[0] += lst\n","            sample2[1] += lst\n","            v1_lst.append(row2[sentence2].split())\n","            v2_lst.append(row2[sentence2].split())\n","            vec_lst1.append([WORD2VEC.wv[word2] for word2 in row2[sentence2].split()])\n","            vec_lst2.append([WORD2VEC.wv[word2] for word2 in row2[sentence2].split()])\n","        sample2[0] += [WORD2VEC.wv[word2] for word2 in row2[5].split()]\n","        sample2[1] += [WORD2VEC.wv[word2] for word2 in row2[6].split()]\n","        v1_lst.append(row2[5].split())\n","        v2_lst.append(row2[6].split())\n","        v1_last = [WORD2VEC.wv[word2] for word2 in row2[5].split()]\n","        v2_last = [WORD2VEC.wv[word2] for word2 in row2[6].split()]\n","        vec_lst1.append([WORD2VEC.wv[word2] for word2 in row2[5].split()])\n","        vec_lst2.append([WORD2VEC.wv[word2] for word2 in row2[6].split()])\n","        v1 = np.stack(sample2[0], axis=0)\n","        v2 = np.stack(sample2[1], axis=0)\n","        v1 = np.expand_dims(v1, axis=0)\n","        v2 = np.expand_dims(v2, axis=0)\n","        if row2[7] == 1:\n","            pos_lst = v1_lst\n","            neg_lst = v2_lst\n","            pos1 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word2] for word2 in row2[4].split()], axis=0), axis=0)).to(DEVICE)\n","            pos2 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word2] for word2 in row2[5].split()], axis=0), axis=0)).to(DEVICE)\n","            neg1 = pos1\n","            neg2 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word2] for word2 in row2[6].split()], axis=0), axis=0)).to(DEVICE)\n","            pos_pair = (pos1, pos2)\n","            neg_pair = (neg1, neg2)\n","            dev_omega.append((pos_pair, neg_pair, 0))\n","        else:\n","            pos_lst = v2_lst\n","            neg_lst = v1_lst\n","            pos1 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word2] for word2 in row2[4].split()], axis=0), axis=0)).to(DEVICE)\n","            pos2 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word2] for word2 in row2[6].split()], axis=0), axis=0)).to(DEVICE)\n","            neg1 = pos1\n","            neg2 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word2] for word2 in row2[5].split()], axis=0), axis=0)).to(DEVICE)\n","            pos_pair = (pos1, pos2)\n","            neg_pair = (neg1, neg2)\n","            dev_omega.append((pos_pair, neg_pair, 1))\n","        v1_last = np.stack(v1_last, axis=0)\n","        v1_last = np.expand_dims(v1_last, axis=0)\n","        v2_last = np.stack(v2_last, axis=0)\n","        v2_last = np.expand_dims(v2_last, axis=0)\n","        val1 = (torch.from_numpy(v1).to(DEVICE), v1_lst, torch.from_numpy(v1_last).to(DEVICE), vec_lst1)\n","        val2 = (torch.from_numpy(v2).to(DEVICE), v2_lst, torch.from_numpy(v2_last).to(DEVICE), vec_lst2)\n","        development_data.append((val1, val2, row2[7]-1))\n","        development_data_prime[0].append(pos_lst)\n","        development_data_prime[1].append(neg_lst)\n","\n","    testing_data = []\n","    for row3 in test_data:\n","        sample3 = [[],[]]\n","        t1_lst = []\n","        t2_lst = []\n","        for sentence3 in range(1, 5):\n","            lst = [WORD2VEC.wv[word3] for word3 in row3[sentence3].split()]\n","            sample3[0] += lst\n","            sample3[1] += lst\n","            t1_lst.append(row3[sentence3].split())\n","            t2_lst.append(row3[sentence3].split())\n","        sample3[0] += [WORD2VEC.wv[word3] for word3 in row3[5].split()]\n","        sample3[1] += [WORD2VEC.wv[word3] for word3 in row3[6].split()]\n","        t1_lst.append(row3[5].split())\n","        t2_lst.append(row3[6].split())\n","        t1 = np.stack(sample3[0], axis=0)\n","        t2 = np.stack(sample3[1], axis=0)\n","        t1 = np.expand_dims(t1, axis=0)\n","        t2 = np.expand_dims(t2, axis=0)\n","        testing_data.append(((torch.from_numpy(t1).to(DEVICE), t1_lst), (torch.from_numpy(t2).to(DEVICE), t2_lst), row3[0]))\n","\n","    return training_data, development_data, testing_data, training_data_prime, development_data_prime, train_omega, dev_omega"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ALB5RTLzscjy","colab":{}},"source":["def replace_sparse_words(train_data, dev_data, test_data):\n","\n","    train_data = train_data.to_numpy()\n","    dev_data = dev_data.to_numpy()\n","    test_data = test_data.to_numpy()\n","    # Count words\n","\n","    seen_vocab = {}\n","    for row in train_data:\n","        for i in range(1, 7):\n","            for word in row[i].split():\n","                if seen_vocab.get(word) is None:\n","                    seen_vocab[word] = 1\n","                else:\n","                    seen_vocab[word] += 1\n","    for row in dev_data:\n","        for i in range(1, 7):\n","             for word in row[i].split():\n","                if seen_vocab.get(word) is None:\n","                    seen_vocab[word] = 1\n","                else:\n","                    seen_vocab[word] += 1\n","    for row in test_data:\n","        for i in range(1, 7):\n","             for word in row[i].split():\n","                if seen_vocab.get(word) is None:\n","                    seen_vocab[word] = 1\n","                else:\n","                    seen_vocab[word] += 1\n","\n","    # Replace words\n","    new_train = []\n","    for row in train_data:\n","        new_sample = []\n","        for i in range(1, 7):\n","            new_sentence = []\n","            for word in row[i].split():\n","                new_sentence.append(word if seen_vocab[word] > 3 else '<UNK>') \n","            new_sample.append(new_sentence)\n","        new_sample.insert(0, row[0])\n","        new_sample.append(row[7])\n","        new_train.append(new_sample)\n","\n","    new_dev = []\n","    for row in dev_data:\n","        new_sample = []\n","        for i in range(1, 7):\n","            new_sentence = []\n","            for word in row[i].split():\n","                new_sentence.append(word if seen_vocab[word] > 3 else '<UNK>') \n","            new_sample.append(new_sentence)\n","        new_sample.insert(0, row[0])\n","        new_sample.append(row[7])\n","        new_dev.append(new_sample)\n","\n","    new_test = []\n","    for row in test_data:\n","        new_sample = []\n","        for i in range(1, 7):\n","            new_sentence = []\n","            for word in row[i].split():\n","                new_sentence.append(word if seen_vocab[word] > 3 else '<UNK>') \n","            new_sample.append(new_sentence)\n","        new_sample.insert(0, row[0])\n","        new_test.append(new_sample)\n","  \n","    return new_train, new_dev, new_test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"abm3MiiaynMo","colab_type":"code","colab":{}},"source":["def get_training_word_ngrams(data):\n","    sentence_ngrams = {}\n","    for sample in data:\n","        for sentence_index in range(1, 7):\n","            for n in range(1, 6):\n","                ngram_lst = ngrams(sample[sentence_index], n)\n","                for ngram in ngram_lst:\n","                    if sentence_ngrams.get(ngram) is None:\n","                        sentence_ngrams[ngram] = 1\n","                    else:\n","                        sentence_ngrams[ngram] += 1\n","    copy = list(sentence_ngrams.keys())\n","    for ngram_key in copy:\n","        if sentence_ngrams[ngram_key] < 5:\n","            del sentence_ngrams[ngram_key]\n","    \n","    return sentence_ngrams.keys()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zBE6ZiQg9y_d","colab":{}},"source":["train_data = pd.read_csv('./train.csv', encoding='latin-1')\n","dev_data = pd.read_csv('./dev.csv', encoding='latin-1')\n","test_data = pd.read_csv('./test.csv', encoding='latin-1')\n","\n","training_data, development_data, testing_data, training_data_prime, development_data_prime, train_omega, dev_omega = embed(train_data, dev_data, test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aGgSYACWsiMO","colab":{}},"source":["new_train, new_dev, new_test = replace_sparse_words(train_data, dev_data, test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rZKEUhq9yzm1","colab_type":"code","colab":{}},"source":["# training_word_ngrams = get_training_word_ngrams(new_train)\n","\n","# ultimately want input vectors to be counter-like representations of\n","# - the length of the last sentence and\n","# - ngrams (character too eventually but just word for now) in the last sentence that also appear in the training set\n","\n","# - maybe output of recurrent neural network trained on word/POS embeddings using spaCy POS tagger"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ef6bLF1S7nQh","colab":{}},"source":["def check_for_unk_words(wordlist, tokenlist):\n","    for i, token in enumerate(wordlist):\n","        if token not in tokenlist:\n","              wordlist[i] = '<UNK>'\n","    return wordlist\n","\n","\n","def get_unigram_corpus(wordlist):\n","    return dict(Counter(wordlist))\n","\n","\n","def get_bigram_corpus(wordlist):\n","    corpus = {}\n","    for i, word in enumerate(wordlist[1:], start=1):\n","        if word != '<s>':\n","            if (wordlist[i-1], word) not in corpus:\n","                corpus[(wordlist[i-1], word)] = 1\n","            else:\n","                corpus[(wordlist[i-1], word)] += 1\n","    return corpus\n","\n","\n","def get_smooth_bigram_corpus(tokenlist, bigram_corpus):\n","    tokenlist.append('<UNK>')\n","    df = pd.DataFrame(1, index = tokenlist, columns = tokenlist) \n","    for bigram in bigram_corpus:\n","        df.loc[bigram[0], bigram[1]] += bigram_corpus[bigram]\n","    return df\n","\n","\n","def get_smooth_bigram_prob(bigram, smooth_bigram_corpus):\n","    return smooth_bigram_corpus.loc[bigram[0], bigram[1]] / smooth_bigram_corpus.loc[bigram[0]].sum()\n","\n","  \n","class NGramModel():\n","    def __init__(self, *args):\n","        super(NGramModel, self).__init__()\n","\n","    def get_perp(self, *args):\n","        return\n","\n","\n","class SmoothBigramModel(NGramModel):\n","    def __init__(self, data):\n","        super(SmoothBigramModel, self).__init__()\n","        data = self.preprocess(data)\n","        self.tokens = list(get_unigram_corpus(data).keys())\n","        corpus = get_bigram_corpus(data)\n","        self.corpus = get_smooth_bigram_corpus(self.tokens, corpus)\n","\n","    def preprocess(self, data):\n","        total = []\n","        for sample in data:\n","            sentences = sample[1]\n","            for sentence in sentences:\n","                total += sentence\n","        return total\n","\n","    def get_perp(self, sentences):\n","        sentences = check_for_unk_words(sentences, self.tokens)\n","        N = len(sentences)\n","        acc = 0\n","        for i, word in enumerate(sentences):\n","            if i == 0:\n","                continue\n","            bigram = (sentences[i-1], word)\n","            acc -= math.log(get_smooth_bigram_prob(bigram, self.corpus))\n","        return math.exp(1/(N-1) * acc)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CkuKxfdZ-SWV","colab":{}},"source":["class bigram_nb_classifier():\n","    def __init__(self, data):\n","        truthful_train = data[0]\n","        false_train = data[1]\n","        truthful_pairs = self.preprocess(truthful_train)\n","        false_pairs = self.preprocess(false_train)\n","        truthful_data = [(self.create_bigram_features(pair), 1) for pair in truthful_pairs]\n","        false_data = [(self.create_bigram_features(pair), 0) for pair in false_pairs]\n","        self.classifier = NaiveBayesClassifier.train(false_data + truthful_data)\n","        self.latest_accuracy = -1\n","\n","    def compute_accuracy(self, val_data):\n","        truthful_val = val_data[0]\n","        false_val = val_data[1]\n","        truthful_pairs = [sentences[3] + sentences[4] for sentences in truthful_val]\n","        false_pairs = [sentences[3] + sentences[4] for sentences in false_val]\n","        truthful_data_v = [(self.create_bigram_features(pair), 1) for pair in truthful_pairs]\n","        false_data_v = [(self.create_bigram_features(pair), 1) for pair in false_pairs]\n","        self.latest_accuracy = nltk.classify.util.accuracy(self.classifier, truthful_data_v + false_data_v )\n","        return self.latest_accuracy\n","    \n","    def create_bigram_features(self, words):\n","        ngram_vocab = ngrams(words, 2)\n","        my_dict = dict([(ng, True) for ng in ngram_vocab])\n","        return my_dict\n","\n","    def preprocess(self, tuple_lst):\n","        story_lst = []\n","        for sample in tuple_lst:\n","            story_lst.append(sample[1])\n","        pair_lst = [sentences[3] + sentences[4] for sentences in story_lst]\n","        return pair_lst\n","    \n","    def classify(self, pair):\n","        return self.classifier.classify(self.create_bigram_features(pair))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-3Gc5cNmsb4w","colab":{}},"source":["TMODEL = SmoothBigramModel(training_data_prime[0])\n","FMODEL = SmoothBigramModel(training_data_prime[1])\n","BNB = bigram_nb_classifier(training_data_prime)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nrN77oT2u8mE","colab":{}},"source":["# print(BNB.compute_accuracy(development_data_prime))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lfN6LODr1le7","colab":{}},"source":["# data = development_data\n","# random.shuffle(data)\n","# correct = 0\n","# total = 0\n","# N = len(data)\n","# for index in tqdm(range(N)):\n","#     input_1, input_2, gold_label = data[index]\n","#     pair_1 = input_1[1][3] + input_1[1][4]\n","#     pair_2 = input_1[1][3] + input_1[1][4]\n","#     prob_truthful_1 = 1 / TMODEL.get_perp(pair_1)\n","#     prob_false_1 = 1 / FMODEL.get_perp(pair_1)\n","#     prob_truthful_2 = 1 / TMODEL.get_perp(pair_2)\n","#     prob_false_2 = 1 / FMODEL.get_perp(pair_2)\n","#     softmax = nn.Softmax(dim=0)\n","#     probs = torch.tensor([prob_truthful_1, prob_false_1, prob_truthful_2, prob_false_2], device=DEVICE)\n","#     probs = softmax(probs)\n","#     max_index = torch.argmax(probs)\n","#     if max_index == 0 or max_index == 3:\n","#         predicted_label = 0\n","#     if max_index == 1 or max_index == 2:\n","#         predicted_label = 1\n","#     correct += int(predicted_label == gold_label)\n","#     total += 1\n","# print(correct / total)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MtBT_1kv8JHf","colab":{}},"source":["class MatchDotCOMP(nn.Module):\n","    def __init__(self):\n","        super(MatchDotCOMP, self).__init__()\n","        # self.lstm = nn.LSTM(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True)\n","        self.gru = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidirectional=True)\n","        self.linear = nn.Linear(HIDDEN_DIM*2, 2)\n","        # self.softmax = nn.LogSoftmax(dim=0)\n","        self.criterion = nn.NLLLoss()\n","        self.cuda(device=DEVICE)\n","\n","    def compute_Loss(self, predicted_vector, gold_label):\n","        # print('MatchDotCOMP: predicted = {}, gold = {}'.format(predicted_vector, gold_label))\n","        return self.criterion(predicted_vector, gold_label)\n","\n","    def forward(self, inputs):\n","        h_0 = torch.zeros((LAYERS*2, 1, HIDDEN_DIM), device=DEVICE)\n","        output, __ = self.gru(inputs, h_0)\n","        # c_0 = h_0.clone()\n","        # output, __ = self.lstm(inputs, (h_0, c_0))\n","        x = output[0][-1]\n","        x = self.linear(x)\n","        # x = self.softmax(x)\n","        return x\n","\n","\n","class Groot(nn.Module):\n","    def __init__(self):\n","        super(Groot, self).__init__()\n","        # self.lstm1 = nn.LSTM(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True)\n","        # self.lstm2 = nn.LSTM(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True)\n","        self.gru1 = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidirectional=True)\n","        self.gru2 = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidirectional=True)\n","        self.linear = nn.Linear(HIDDEN_DIM*2, 2)\n","        # self.softmax = nn.LogSoftmax(dim=0)\n","        self.criterion = nn.NLLLoss()\n","        self.cuda(device=DEVICE)\n","\n","    def compute_Loss(self, predicted_vector, gold_label):\n","        # print('Groot: predicted = {}, gold = {}'.format(predicted_vector, gold_label))\n","        return self.criterion(predicted_vector, gold_label)\n","\n","    def forward(self, inputs):\n","        input1 = []\n","        for i in range(4):\n","            input1 += inputs[i]\n","        input1 = np.stack(input1, axis=0)\n","        input1 = np.expand_dims(input1, axis=0)\n","        input1 = torch.from_numpy(input1).to(DEVICE)\n","        h_0 = torch.zeros((LAYERS*2, 1, HIDDEN_DIM), device=DEVICE)\n","        # c_0 = h_0.clone()\n","        # __, hcn = self.lstm1(input1, (h_0, c_0))\n","        __, h_n = self.gru1(input1, h_0)\n","        input2 = np.expand_dims(inputs[4], axis=0)\n","        input2 = torch.from_numpy(input2).to(DEVICE)\n","        # h_n = hcn[0]\n","        # c_n = hcn[1]\n","        # output, __ = self.lstm2(input2, (h_n, c_n))\n","        output, __ = self.gru2(input2, h_n)\n","        x = output[0][-1]\n","        x = self.linear(x)\n","        # x = self.softmax(x)\n","        return x\n","\n","\n","class Blind(nn.Module):\n","    def __init__(self):\n","        super(Blind, self).__init__()\n","        # self.lstm = nn.LSTM(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True)\n","        self.gru = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidirectional=True)\n","        self.linear = nn.Linear(HIDDEN_DIM*2, 2)\n","        # self.softmax = nn.LogSoftmax(dim=0)\n","        self.criterion = nn.NLLLoss()\n","        self.cuda(device=DEVICE)\n","\n","    def compute_Loss(self, predicted_vector, gold_label):\n","        # print('Blind: predicted = {}, gold = {}'.format(predicted_vector, gold_label))\n","        return self.criterion(predicted_vector, gold_label)\n","\n","    def forward(self, inputs):\n","        h_0 = torch.zeros((LAYERS*2, 1, HIDDEN_DIM), device=DEVICE)\n","        # c_0 = h_0.clone()\n","        # output, __ = self.lstm(inputs, (h_0, c_0))\n","        output, __ = self.gru(inputs, h_0)\n","        x = output[0][-1]\n","        x = self.linear(x)\n","        # x = self.softmax(x)\n","        return x\n","\n","\n","FEATURES = 5\n","# 1: length of last sentence\n","# 2: truthful bigram perplexity of last two sentences\n","# 3: false bigram perplexity of last two sentences\n","# 4: bigram naive bayes truthful/false classification of last two sentences\n","# 5: bigram overlap between last sentence and previous four\n","\n","\n","class LMFeatureExtractor(nn.Module):\n","    def __init__(self):\n","        super(LMFeatureExtractor, self).__init__()\n","        # self.linear = nn.Linear(FEATURES, 2)\n","        # self.softmax = nn.LogSoftmax(dim=0)\n","        self.cuda(device=DEVICE)\n","        self.tmodel = TMODEL\n","        self.fmodel = FMODEL\n","        self.bnb = BNB\n","\n","    def get_bigram_overlap(self, inputs):\n","        first_four = get_bigram_corpus(inputs[0]+inputs[1]+inputs[2]+inputs[3])\n","        last_one = get_bigram_corpus(inputs[4])\n","        total = 0\n","        intersect = 0\n","        for bigram in last_one:\n","            if bigram in first_four:\n","                intersect += 1\n","            total += 1\n","        overlap = intersect / total\n","        return overlap\n","\n","    def extract_features(self, inputs):\n","        last_two = inputs[3] + inputs[4]\n","        features = []\n","        features.append(len(inputs[4]))\n","        features.append(self.tmodel.get_perp(last_two))\n","        features.append(self.fmodel.get_perp(last_two))\n","        features.append(self.bnb.classify(last_two))\n","        features.append(self.get_bigram_overlap(inputs))\n","        return torch.tensor(features, device=DEVICE, dtype=torch.float)\n","\n","    def forward(self, inputs):\n","        x = self.extract_features(inputs)\n","        # x = self.linear(x)\n","        # return self.softmax(x)\n","        return x\n","\n","\n","class RNNFeatureExtractor(nn.Module):\n","    def __init__(self):\n","        super(RNNFeatureExtractor, self).__init__()\n","        # self.lstm = nn.LSTM(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True)\n","        self.gru = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True)\n","        self.linear = nn.Linear(HIDDEN_DIM, 2)\n","        self.softmax = nn.LogSoftmax(dim=0)\n","        self.cuda(device=DEVICE)\n","\n","    def forward(self, inputs):\n","        h_0 = torch.zeros(LAYERS, 1, HIDDEN_DIM, device=DEVICE)\n","        # output, __ = self.gru(inputs, h_0)\n","        c_0 = h_0.clone()\n","        output, __ = self.lstm(inputs, (h_0, c_0))\n","        x = output[0][-1]\n","        x = self.linear(x)\n","        x = self.softmax(x)\n","        return x\n","\n","\n","class RNNLM(nn.Module):\n","    def __init__(self):\n","        super(RNNLM, self).__init__()\n","        # self.rnn_fe = RNNFeatureExtractor()     # P(ending ^ story) ?\n","        self.lm_fe = LMFeatureExtractor()\n","        # self.blind = Blind()                    # P(ending) ?\n","        # self.groot = Groot()                    # P(ending | story) ?\n","        self.linear = nn.Linear(FEATURES, 2)\n","        self.softmax = nn.LogSoftmax(dim=0)\n","        self.criterion = nn.NLLLoss()\n","        # self.optimizer = optim.SGD(self.parameters(), lr=LR, momentum=0.9)\n","        # self.optimizer = optim.SGD(self.parameters(), lr=LR, momentum=0.9)\n","        # self.optimizer = optim.Adam(self.parameters(), lr=LR)\n","        self.optimizer = optim.RMSprop(self.parameters(), lr=LR)\n","        # self.optimizer = adabound.AdaBound(self.parameters(), lr=LR, final_lr=0.01)\n","        self.cuda(device=DEVICE)\n","\n","    def compute_Loss(self, predicted_vector, gold_label):\n","        return self.criterion(predicted_vector, gold_label)\n","\n","    def forward(self, inputs):\n","        # rnn_features = self.rnn_fe(inputs[0])\n","        lm_features = self.lm_fe(inputs[1])\n","        # blind_features = self.blind(inputs[2])\n","        # groot_features = self.groot(inputs[3])\n","        # features = torch.cat((rnn_features, blind_features, groot_features), dim=0)\n","        # features = torch.cat((rnn_features, lm_features, blind_features, groot_features), dim=0)\n","        # features = torch.tensor([math.exp(blind_features[1]), math.exp(groot_features[1]), math.exp(blind_features[1]) / math.exp(groot_features[1])], device=DEVICE)\n","        # features = torch.cat((blind_features, groot_features), dim=0)\n","        x = self.linear(lm_features)\n","        x = self.softmax(x)\n","        return x\n","\n","# previously: BigBand\n","# this was dumb\n","class BADBand(nn.Module):\n","    def __init__(self):\n","        super(BigBand, self).__init__()\n","        self.p_ending = Blind()\n","        self.p_ending_given_story = Groot()\n","        self.P_ending_and_story = MatchDotCOMP()\n","        self.optimizer = optim.RMSprop(self.parameters(), lr=LR)\n","        self.pv1 = None\n","        self.pv2 = None\n","        self.pv3 = None\n","        self.cuda(device=DEVICE)\n","\n","    def compute_Loss(self, gold_label):\n","        loss1 = self.p_ending.compute_Loss(self.pv1.view(1, -1), gold_label)\n","        loss2 = self.p_ending_given_story.compute_Loss(self.pv2.view(1, -1), gold_label)\n","        loss3 = self.P_ending_and_story.compute_Loss(self.pv3.view(1, -1), gold_label)\n","        return loss1, loss2, loss3\n","\n","    def forward(self, inputs):\n","        pv1 = self.p_ending(inputs[2])\n","        pv2 = self.p_ending_given_story(inputs[3])\n","        pv3 = self.P_ending_and_story(inputs[0])\n","        self.pv1 = pv1\n","        self.pv2 = pv2\n","        self.pv3 = pv3\n","        label1 = torch.argmax(pv1)\n","        label2 = torch.argmax(pv2)\n","        label3 = torch.argmax(pv3)\n","        lst = [label1, label2, label3]\n","        x = max(set(lst), key=lst.count)\n","        return x\n","\n","\n","class JAZZBand(nn.Module):\n","    def __init__(self):\n","        super(JAZZBand, self).__init__()\n","        self.saxophone = Blind()\n","        self.trumpet = Groot()\n","        self.drums = MatchDotCOMP()\n","        # self.JAZZ = nn.Linear(HIDDEN_DIM*2, 2)\n","        self.encore = nn.LogSoftmax(dim=0)\n","        self.criterion = nn.NLLLoss()\n","        self.optimizer = optim.RMSprop(self.parameters(), lr=LR)\n","        self.cuda(device=DEVICE)\n","\n","    def compute_Loss(self, predicted_vector, gold_label):\n","        return self.criterion(predicted_vector, gold_label)\n","\n","    def forward(self, inputs):\n","        sax = self.saxophone(inputs[2])\n","        trump = self.trumpet(inputs[3])\n","        dr = self.drums(inputs[0])\n","        x = sax + trump + dr\n","        # x = self.JAZZ(x)\n","        x = self.encore(x)\n","        return x\n","\n","\n","class NSP(nn.Module):\n","    def __init__(self):\n","        super(NSP, self).__init__()\n","        self.gru1 = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidrectional=True)\n","        self.gru2 = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidrectional=True)\n","        self.linear = nn.Linear(HIDDEN_DIM*2, 2)\n","        self.softmax = nn.LogSoftmax(dim=0)\n","        self.criterion = nn.NLLLoss()\n","        self.optimizer = optim.Adam(self.parameters(), lr=LR)\n","        self.cuda(device=DEVICE)\n","\n","    def compute_Loss(self, predicted_vector, gold_label):\n","        return self.criterion(predicted_vector, gold_label)\n","\n","    def forward(self, inputs):\n","        h_0 = torch.zeros((LAYERS, 1, HIDDEN_DIM), device=DEVICE)\n","        __, h_n = self.gru1(inputs[0], h_0)\n","        output, __ = self.gru2(inputs[1], h_n)\n","        x = output[0][-1]\n","        x = self.linear(x)\n","        x = self.softmax(x)\n","        return x\n","\n","\n","class LM(nn.Module):\n","    def __init__(self):\n","        super(LM, self).__init__()\n","        self.word_ngrams = get_training_word_ngrams(new_train)\n","        self.linear = nn.Linear(len(self.word_ngrams) + 1, 2)\n","        self.softmax = nn.LogSoftmax(dim=0)\n","        self.criterion = nn.NLLLoss()\n","        self.optimizer = optim.Adam(self.parameters(), lr=LR)\n","        self.cuda(device=DEVICE)\n","    \n","    def compute_Loss(self, predicted_vector, gold_label):\n","        return self.criterion(predicted_vector, gold_label)\n","    \n","    def extract_features(self, sentence):\n","        features = [len(sentence)]\n","        sentence_ngrams = {}\n","        for n in range(1, 6):\n","            ngram_lst = ngrams(sentence, n)\n","            for ngram in ngram_lst:\n","                if sentence_ngrams.get(ngram) is None:\n","                    sentence_ngrams[ngram] = 1\n","                else:\n","                    sentence_ngrams[ngram] += 1\n","        for key in list(sentence_ngrams.keys()):\n","            if key not in self.word_ngrams:\n","                del sentence_ngrams[key]\n","        for key in list(self.word_ngrams):\n","            features.append(sentence_ngrams.get(key, 0))\n","        return torch.tensor(features, device=DEVICE, dtype=torch.float)\n","\n","    def forward(self, inputs):\n","        last = inputs[4]\n","        feature = self.extract_features(last)\n","        x = self.linear(feature)\n","        x = self.softmax(x)\n","        return x\n","\n","    \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"error","timestamp":1575343088440,"user_tz":300,"elapsed":275,"user":{"displayName":"Daniel Parangi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64","userId":"01211561534677666164"}},"id":"YQmeLmMa95Ac","outputId":"caf9fa70-51d2-40ee-e386-662ce37dd000","colab":{"base_uri":"https://localhost:8080/","height":868}},"source":["print('Initializing Model')\n","model = LM()\n","prev_dev_acc = 0.0\n","for epoch in range(EPOCHS):\n","    checkpoint = PATH + '-e' + str((epoch + 1))\n","    model.train()\n","    model.optimizer.zero_grad()\n","    loss = None\n","    correct = 0\n","    total = 0\n","    start_time = time.time()\n","    print('Training started for epoch {}'.format(epoch + 1))\n","    random.shuffle(new_train)\n","    # random.shuffle(training_data)\n","    # random.shuffle(train_omega)\n","    N = len(new_train)\n","    # N = len(training_data)\n","    # N = len(train_omega)\n","    for index  in tqdm(range(N)):\n","        model.optimizer.zero_grad()\n","        sample = new_train[index]\n","        input_vector = sample[:7]\n","        gold_label = sample[7]\n","        # input_vector, gold_label = training_data[index]\n","        # input_vector, gold_label = train_omega[index]\n","        predicted_vector = model(input_vector)\n","        predicted_label = torch.argmax(predicted_vector)\n","        correct += int(predicted_label == gold_label)\n","        total += 1\n","        loss = model.compute_Loss(predicted_vector.view(1, -1), torch.tensor([gold_label], device=DEVICE))\n","        loss.backward()\n","        model.optimizer.step()\n","    print('Training accuracy for epoch {}: {}'.format(epoch + 1, correct / total))\n","    correct = 0\n","    total = 0\n","    start_time = time.time()\n","    random.shuffle(new_dev)\n","    # random.shuffle(development_data)\n","    # random.shuffle(dev_omega)\n","    N = len(new_dev)\n","    # N = len(development_data)\n","    # N = len(dev_omega)\n","    model.eval()\n","    model.optimizer.zero_grad()\n","    for index in tqdm(range(N)):\n","        sample = new_dev[index]\n","        input_1 = new_dev[:6]\n","        input_2 = new_dev[:5] + new_dev[6]\n","        gold_label = new_dev[7]\n","        # input_1, input_2, gold_label = development_data[index]\n","        # input_1, input_2, gold_label = dev_omega[index]\n","        prediction_1 = model(input_1)\n","        prediction_2 = model(input_2)\n","        prob_truthful_1 = prediction_1[1]\n","        prob_false_1 = prediction_1[0]\n","        prob_truthful_2 = prediction_2[1]\n","        prob_false_2 = prediction_2[0]\n","        probs = [prob_truthful_1, prob_false_1, prob_truthful_2, prob_false_2]\n","        max_index = probs.index(max(probs))\n","        if max_index == 0 or max_index == 3:\n","            predicted_label = 0\n","        if max_index == 1 or max_index == 2:\n","            predicted_label = 1\n","        correct += int(predicted_label == gold_label)\n","        total += 1\n","    dev_acc = correct / total\n","    if dev_acc > prev_dev_acc and dev_acc > 0.67:\n","        prev_dev_acc = dev_acc\n","        print('New Best Accuracy: {}'.format(dev_acc))\n","        acc = int(100 * dev_acc)\n","        torch.save(model.state_dict(), checkpoint + '-a' + str(acc) + '.pt')\n","    print('Development accuracy for epoch {}: {}'.format(epoch + 1, correct / total))\n","\n","torch.save(model.state_dict(), PATH + '.pt')"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Initializing Model\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-fb78d48cd02e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Initializing Model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNNLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprev_dev_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-e'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-39-ad97f641763b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# self.optimizer = adabound.AdaBound(self.parameters(), lr=LR, final_lr=0.01)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \"\"\"\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \"\"\"\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SVtFlIjDJg09","colab":{}},"source":["# Test : Kaggle A\n","# model = RNNLM()\n","# model.load_state_dict(torch.load(os.path.join(MODELS, 'rnnlm-baseline-e10-a67.pt')))\n","\n","# N = len(testing_data)\n","# ids = []\n","# predictions = []\n","# for index in tqdm(range(N)):\n","#     input_1, input_2, id_tag = testing_data[index]\n","#     prediction_1 = model(input_1)\n","#     prediction_2 = model(input_2)\n","\n","#     prob_truthful_1 = prediction_1[1]\n","#     prob_false_1 = prediction_1[0]\n","#     prob_truthful_2 = prediction_2[1]\n","#     prob_false_2 = prediction_2[0]\n","\n","#     probs = [prob_truthful_1, prob_false_1, prob_truthful_2, prob_false_2]\n","\n","#     max_index = probs.index(max(probs))\n","#     if max_index == 0 or max_index == 3:\n","#         predicted_label = 0\n","#     if max_index == 1 or max_index == 2:\n","#         predicted_label = 1\n","#     ids.append(id_tag)\n","#     predictions.append(predicted_label + 1)\n","\n","# df = pd.DataFrame({'Id': ids, 'Prediction': predictions}, columns = ['Id', 'Prediction'])\n","# df.to_csv('Part-A_rnnlm-baseline-e10-a67.csv', index=False)"],"execution_count":0,"outputs":[]}]}