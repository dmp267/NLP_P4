{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PartBWorking_gpu.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"EytCBDLKhwaw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"20fc3776-bb7f-4d07-bc62-0ac82a946ec1","executionInfo":{"status":"ok","timestamp":1575865642775,"user_tz":300,"elapsed":966,"user":{"displayName":"Daniel Parangi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64","userId":"01211561534677666164"}}},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/My Drive/College/F19/CS 4740/NLP_P4"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n","/gdrive/My Drive/College/F19/CS 4740/NLP_P4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M6r1uobfw3t7","colab_type":"code","colab":{}},"source":["# %pip install transformers"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tlGDHGSbh-DS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":134},"outputId":"ddfc580d-3f01-4a2e-dada-fea7ce3df948","executionInfo":{"status":"ok","timestamp":1575865648654,"user_tz":300,"elapsed":6821,"user":{"displayName":"Daniel Parangi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64","userId":"01211561534677666164"}}},"source":["import numpy as np\n","import pandas as pd\n","import csv\n","import torch\n","import torch.nn as nn\n","from torch.nn import init\n","import torch.optim as optim\n","import random\n","import os\n","import time\n","from tqdm import tqdm\n","\n","train_data = pd.read_csv('./gpu_train.csv', encoding='latin-1')\n","dev_data = pd.read_csv('./gpu_dev.csv', encoding='latin-1')\n","test_data = pd.read_csv('./gpu_test.csv', encoding='latin-1')\n","data = [train_data, dev_data, test_data]\n","bert = torch.hub.load('huggingface/transformers', 'model', 'bert-base-uncased')\n","tokenizer = torch.hub.load('huggingface/transformers', 'tokenizer', 'bert-base-uncased')\n","\n","EPOCHS = 1\n","LR = 0.001\n","DEVICE = torch.device(\"cuda:0\")\n","NAME = 'bert1'\n","CURRENT = os.curdir\n","MODELS = os.path.join(CURRENT, 'experimental_models')\n","PATH = os.path.join(MODELS, NAME)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_transformers_master\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_transformers_master\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"dxRuwLmHiILC","colab_type":"code","colab":{}},"source":["def train_and_classify(training_data, development_data):\n","    class BertnaryClassification(nn.Module):\n","        def __init__(self):\n","            super(BertnaryClassification, self).__init__()\n","            self.linear = nn.Linear(768, 2)\n","            self.softmax = nn.LogSoftmax(dim=0)\n","            self.criterion = nn.NLLLoss()\n","            self.optimizer = optim.Adam(self.parameters(), lr=LR)\n","            self.cuda(device=DEVICE)\n","\n","        def compute_Loss(self, predicted_vector, gold_label):\n","            return self.criterion(predicted_vector, gold_label)\n","\n","        def forward(self, input_vector):\n","            features = torch.mean(bert(input_vector)[0].squeeze(), dim=0).to(DEVICE)\n","            prediction = self.linear(features)\n","            return self.softmax(prediction)\n","\n","    def setup(training_data, development_data):\n","        print('Initializing Setup')\n","        train_data = []\n","        for row in training_data.iterrows():\n","            ID = row[1][0]\n","            if row[1][7] == 1:\n","                pos_story = ' '.join(w for w in [word for sentence in row[1][1:6] for word in sentence])\n","                pos_story = torch.tensor([tokenizer.encode(pos_story, add_special_tokens=True)])\n","                pos = (ID, pos_story, 1)\n","                neg_story = ' '.join(w for w in [word for sentence in row[1][1:5] + [row[1][6]] for word in sentence])\n","                neg_story = torch.tensor([tokenizer.encode(neg_story, add_special_tokens=True)])\n","                neg = (ID, neg_story, 0)\n","            else:\n","                neg_story = ' '.join(w for w in [word for sentence in row[1][1:6] for word in sentence])\n","                neg_story = torch.tensor([tokenizer.encode(neg_story, add_special_tokens=True)])\n","                neg = (ID, neg_story, 0)\n","                pos_story = ' '.join(w for w in [word for sentence in row[1][1:5] + [row[1][6]] for word in sentence])\n","                pos_story = torch.tensor([tokenizer.encode(pos_story, add_special_tokens=True)])\n","                pos = (ID, pos_story, 1)\n","            train_data.append(pos)\n","            train_data.append(neg)\n","\n","        dev_data = []\n","        for row in development_data.iterrows():\n","            ID = row[1][0]\n","            LABEL = row[1][7]\n","            story_1 = ' '.join(w for w in [word for sentence in row[1][1:6] for word in sentence])\n","            story_1 = torch.tensor([tokenizer.encode(story_1, add_special_tokens=True)], device=DEVICE, dtype=torch.float)\n","            story_2 = ' '.join(w for w in [word for sentence in row[1][1:5] + [row[1][6]] for word in sentence])\n","            story_2 = torch.tensor([tokenizer.encode(story_2, add_special_tokens=True)], device=DEVICE, dtype=torch.float)\n","            sample = (ID, story_1, story_2, LABEL)\n","            dev_data.append(sample)\n","\n","        return train_data, dev_data\n","\n","    def train(train_data):\n","        model = BertnaryClassification()\n","        for epoch in range(EPOCHS):\n","            model.train()\n","            model.optimizer.zero_grad()\n","            loss = None\n","            correct = 0\n","            total = 0\n","            random.shuffle(train_data)\n","            N = len(train_data)\n","            print('Training...')\n","            for index in tqdm(range(N)):\n","                model.optimizer.zero_grad()\n","                __, input_vector, gold_label = train_data[index]\n","                predicted_vector = model(input_vector)\n","                predicted_label = torch.argmax(predicted_vector)\n","                loss = model.compute_Loss(predicted_vector.view(1, -1), torch.tensor([gold_label], device=DEVICE))\n","                loss.backward()\n","                model.optimizer.step()\n","        return model\n","\n","    def validate(dev_data, model):\n","        model.eval()\n","        model.optimizer.zero_grad()\n","        N = len(dev_data)\n","        labels = []\n","        print('Validating...')\n","        for index in tqdm(range(N)):\n","            __, input_1, input_2, __ = dev_data[index]\n","            prediction_1 = model(input_1)\n","            prediction_2 = model(input_2)\n","            prob_truthful_1 = prediction_1[1]\n","            prob_false_1 = prediction_1[0]\n","            prob_truthful_2 = prediction_2[1]\n","            prob_false_2 = prediction_2[0]\n","            probs = [prob_truthful_1, prob_false_1, prob_truthful_2, prob_false_2]\n","            max_index = probs.index(max(probs))\n","            if max_index == 0 or max_index == 3:\n","                predicted_label = 0\n","            if max_index == 1 or max_index == 2:\n","                predicted_label = 1\n","            labels.append(predicted_label)\n","    \n","    def accuracy(predictions, dev_data):\n","        correct = 0\n","        total = 0\n","        for i in range(len(predictions)):\n","            correct += int(predictions[i] == dev_data[i][3])\n","            total += 1\n","        return correct / total\n","\n","    train_data, dev_data = setup(training_data, development_data)\n","\n","    # create and train model\n","    model = train(train_data)\n","\n","    # evaulate model\n","    predictions = validate(dev_data, model)\n","    acc = accuracy(predictions, dev_data)\n","    return 'accurary: ' + str(round(acc * 100, 2)) + '%'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JD0nIrxPjAvd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"ff284678-174c-458a-eb97-ef34bfc527ab"},"source":["train_and_classify(train_data, dev_data)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initializing Setup\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/2994 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training...\n"],"name":"stdout"},{"output_type":"stream","text":["  6%|â–‹         | 191/2994 [08:21<1:55:20,  2.47s/it]"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Kh8LSNMaxWi6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}