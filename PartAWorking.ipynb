{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"PartAWorking.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"3qHZm9Eg-rSy","colab_type":"code","outputId":"beef6938-925f-477a-cf8c-ef58346a294f","colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"status":"ok","timestamp":1575933318527,"user_tz":300,"elapsed":34911,"user":{"displayName":"Daniel Parangi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64","userId":"01211561534677666164"}}},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/My Drive/College/F19/CS 4740/NLP_P4"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n","/gdrive/My Drive/College/F19/CS 4740/NLP_P4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"24sZTlRs8tUd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"437c9e2b-8bae-4679-f220-5fb0131dcd20","executionInfo":{"status":"ok","timestamp":1575933379776,"user_tz":300,"elapsed":6101,"user":{"displayName":"Daniel Parangi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64","userId":"01211561534677666164"}}},"source":["# %pip install vaderSentiment"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Collecting vaderSentiment\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/9e/c53e1fc61aac5ee490a6ac5e21b1ac04e55a7c2aba647bb8411c9aadf24e/vaderSentiment-3.2.1-py2.py3-none-any.whl (125kB)\n","\r\u001b[K     |██▋                             | 10kB 16.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 71kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.7MB/s \n","\u001b[?25hInstalling collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.2.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KkjG7YYYwI39","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"JNOldjfpkFxD","colab_type":"text"},"source":["word length, POS, word embeddings"]},{"cell_type":"code","metadata":{"id":"suam4b4K5FEF","colab_type":"code","outputId":"af770dc3-5293-4ffc-e53b-311ce930d134","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1575933385560,"user_tz":300,"elapsed":6222,"user":{"displayName":"Daniel Parangi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64","userId":"01211561534677666164"}}},"source":["import os\n","import math\n","import time\n","import nltk\n","import random\n","import spacy\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.nn import init\n","from nltk.tokenize import word_tokenize\n","from gensim.models import Word2Vec\n","from collections import Counter\n","from tqdm import tqdm\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","\n","nltk.download('punkt')\n","get_pos = spacy.load(\"en_core_web_sm\")\n","analyser = SentimentIntensityAnalyzer()\n","train_data = pd.read_csv('./gpu_train.csv', encoding='latin-1')\n","dev_data = pd.read_csv('./gpu_dev.csv', encoding='latin-1')\n","test_data = pd.read_csv('./gpu_test.csv', encoding='latin-1')\n","all_data = [train_data, dev_data, test_data]\n","\n","PATH = './'\n","\n","EMBED_DIM = 32\n","HIDDEN_DIM = 16\n","LAYERS = 1\n","EPOCHS = 10\n","LR = 3e-4\n","DEVICE = torch.device(\"cuda:0\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IyQEYCMM5FEX","colab_type":"code","colab":{}},"source":["def get_sentiment_scores(sentence):\n","    '''\n","    [negative, neutral, positive, compound]\n","    '''\n","    return [analyser.polarity_scores(sentence)[sentiment] \\\n","            for sentiment in analyser.polarity_scores(sentence).keys()]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eG_04fPKftzp","colab_type":"code","colab":{}},"source":["def get_character_n_grams(sentence, n):\n","    return [sentence[i:i+n] for i in range(len(sentence)-n+1)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h_GVxlfx5FEx","colab_type":"code","colab":{}},"source":["list_of_sentences = []\n","character_4_grams = []\n","def tag_pos(data):\n","    all_pos = []\n","    pos_counts = Counter()\n","\n","    for df in data:\n","        df_pos = []\n","        for row in df.iterrows():\n","            row_pos = []\n","            for i in range(1, 7):\n","                parts_of_speech = get_pos(row[1][i])\n","                \n","                list_of_sentences.append([pos.text for pos in parts_of_speech])\n","                character_4_grams.append(get_character_n_grams(row[1][i], 4))\n","                sentence_pos = [pos.pos_ for pos in parts_of_speech]\n","#                 sentence_pos = [row[1][i], sentence_pos] #temporary\n","                row_pos.append(sentence_pos)\n","                pos_counts.update(sentence_pos)\n","            df_pos.append(row_pos)\n","        all_pos.append(df_pos)\n","        \n","#         \n","    return all_pos, pos_counts"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vpJnQpyr5FE7","colab_type":"code","colab":{}},"source":["pos_data = tag_pos(all_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PIfMuuoj0-H-","colab_type":"code","outputId":"0e3f4021-1817-451b-d378-f627dbe7c959","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1575933589611,"user_tz":300,"elapsed":205060,"user":{"displayName":"Daniel Parangi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64","userId":"01211561534677666164"}}},"source":["print(list_of_sentences[0])\n","print(pos_data[0][0][0])"],"execution_count":13,"outputs":[{"output_type":"stream","text":["['Rick', 'grew', 'up', 'in', 'a', 'troubled', 'household', '.']\n","[['PROPN', 'VERB', 'PART', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT'], ['PRON', 'ADV', 'VERB', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'CCONJ', 'VERB', 'ADP', 'NOUN', 'PUNCT'], ['PRON', 'VERB', 'ADV', 'ADJ', 'ADP', 'PROPN', 'VERB', 'VERB', 'ADP', 'DET', 'NOUN', 'PUNCT'], ['DET', 'NOUN', 'VERB', 'PRON', 'PART', 'VERB', 'DET', 'ADJ', 'NOUN', 'PUNCT'], ['PRON', 'VERB', 'ADJ', 'ADV', 'PUNCT'], ['PRON', 'VERB', 'DET', 'NOUN', 'PUNCT']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"il-17RMAgxxE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WlD7OeVX0Z8M","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"lqeJfOK85FFB","colab_type":"code","outputId":"e08f956d-5f9e-4426-f535-e709ce07293a","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1575933593489,"user_tz":300,"elapsed":206774,"user":{"displayName":"Daniel Parangi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64","userId":"01211561534677666164"}}},"source":["model = Word2Vec(list_of_sentences, size=EMBED_DIM, min_count=1)\n","name = 'spacy_word2vec' + str(EMBED_DIM) + '.model'\n","model.save(name)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"b56J0A6zg84J","colab_type":"code","outputId":"fa569aec-6393-439c-f780-5bd120b4346d","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1575933606686,"user_tz":300,"elapsed":218928,"user":{"displayName":"Daniel Parangi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64","userId":"01211561534677666164"}}},"source":["## n-gram character \n","model = Word2Vec(character_4_grams, size=EMBED_DIM, min_count=1)\n","name = 'n_gram_char_spacy_word2vec' + str(EMBED_DIM) + '.model'\n","model.save(name)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"cl-2JSezAMY7","colab_type":"code","outputId":"4ae554e1-1576-4689-f9fe-29123ec1937f","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1575933606688,"user_tz":300,"elapsed":218291,"user":{"displayName":"Daniel Parangi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64","userId":"01211561534677666164"}}},"source":["w2v = 'spacy_word2vec' + str(EMBED_DIM) + '.model'\n","WORD2VEC = Word2Vec.load(w2v)\n","c2v = 'n_gram_char_spacy_word2vec' + str(EMBED_DIM) + '.model'\n","CHAR2VEC = Word2Vec.load(c2v)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"nAiqnf9gS8W_","colab_type":"code","colab":{}},"source":["def new_word_tokenize(sentence):\n","    return [pos.text for pos in get_pos(sentence)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_pELXASv5FFP","colab_type":"code","colab":{}},"source":["def get_one_hot(group, row, sentence, word):\n","#     print((group, row, sentence, word))\n","    pos = pos_data[0][group][row][sentence][word]\n","#     pos = pos_data[group][row][sentence][word]\n","#     print(pos)\n","    \n","    return [1 if pos == list(pos_data[1].keys())[i] else 0 for i in range(len(pos_data[1].keys()))]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c9xUUjvZSZ0E","colab_type":"code","colab":{}},"source":["def create_vector(i, j, row, sentence, word, group):\n","    return np.array(list(WORD2VEC.wv[word]) + get_one_hot(group, row[0], i-1, j))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mc1jQK7B5FFn","colab_type":"code","colab":{}},"source":["# def embed(train_data, dev_data, test_data):\n","#     training_data = [] \n","#     for row in train_data.iterrows():\n","#         sentences = [row[1][0]]\n","#         char_4_grams = []\n","#         for i in range(1, 7):\n","#             lst = [create_vector(i, j, row, row[1][i], word, 0) \\\n","#                    for j, word in enumerate(new_word_tokenize(row[1][i]))]\n","#             sentences.append(lst)\n","#             char_4_gram_lst = get_character_n_grams(row[1][i], 4)\n","#             char_4_gram_lst = [np.array(list(WORD2VEC.wv[word]) \\\n","#                    for j, word in enumerate(char_4_gram_lst)]\n","                   \n","#         sentences.append(row[1][7])\n","#         sample = (sentences, char_4_grams)\n","#         training_data.append(sentences)\n","    \n","#     development_data = []\n","#     for row in dev_data.iterrows():\n","#         sentences = [row[1][0]]\n","#         for i in range(1, 7):\n","#             lst = [create_vector(i, j, row, row[1][i], word, 1) \\\n","#                    for j, word in enumerate(new_word_tokenize(row[1][i]))]\n","#             sentences.append(lst)\n","#         sentences.append(row[1][7])\n","#         development_data.append(sentences)\n","        \n","#     testing_data = []\n","#     for row in test_data.iterrows():\n","#         sentences = [row[1][0]]\n","#         for i in range(1, 7):\n","#             lst = [create_vector(i, j, row, row[1][i], word, 2) \\\n","#                    for j, word in enumerate(new_word_tokenize(row[1][i]))]\n","#             sentences.append(lst)\n","#         testing_data.append(sentences)\n","        \n","#     return training_data, development_data, testing_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"whCPMZnRfUwf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nNe7dxQ6bxll","colab_type":"code","colab":{}},"source":["def new_embed(train_data, dev_data, test_data):\n","    training_data = [] \n","    for row in train_data.iterrows():\n","        pos = [row[1][0]]\n","        neg = [row[1][0]]\n","        pos_n_grams = []\n","        neg_n_grams = []\n","        for i in range(1, 5):\n","            lst = [create_vector(i, j, row, row[1][i], word, 0) \\\n","                    for j, word in enumerate(new_word_tokenize(row[1][i]))]\n","            pos.append(lst)\n","            neg.append(lst)\n","            n_gram_lst = [np.array(list(CHAR2VEC.wv[char])) for char in get_character_n_grams(row[1][i], 4)]\n","            pos_n_grams.append(n_gram_lst)\n","            neg_n_grams.append(n_gram_lst)               \n","        if row[1][7] == 1:\n","            pos.append([np.array(list(WORD2VEC.wv[word]) + get_one_hot(0, row[0], 4, j)) \\\n","                   for j, word in enumerate(new_word_tokenize(row[1][5]))])\n","            neg.append([np.array(list(WORD2VEC.wv[word]) + get_one_hot(0, row[0], 5, j)) \\\n","                   for j, word in enumerate(new_word_tokenize(row[1][6]))])\n","            pos_n_grams.append([np.array(list(CHAR2VEC.wv[char])) for char in get_character_n_grams(row[1][5], 4)])\n","            neg_n_grams.append([np.array(list(CHAR2VEC.wv[char])) for char in get_character_n_grams(row[1][6], 4)])\n","        elif row[1][7] == 2:\n","            pos.append([np.array(list(WORD2VEC.wv[word]) + get_one_hot(0, row[0], 5, j)) \\\n","                   for j, word in enumerate(new_word_tokenize(row[1][6]))])\n","            neg.append([np.array(list(WORD2VEC.wv[word]) + get_one_hot(0, row[0], 4, j)) \\\n","                   for j, word in enumerate(new_word_tokenize(row[1][5]))])\n","            pos_n_grams.append([np.array(list(CHAR2VEC.wv[char])) for char in get_character_n_grams(row[1][6], 4)])\n","            neg_n_grams.append([np.array(list(CHAR2VEC.wv[char])) for char in get_character_n_grams(row[1][5], 4)])\n","        pos.append(1)\n","        neg.append(0)\n","        training_data.append((pos, pos_n_grams))\n","        training_data.append((neg, neg_n_grams))\n","    \n","    development_data = []\n","    for row in dev_data.iterrows():\n","        ngrams = []\n","        sentences = [row[1][0]]\n","        for i in range(1, 7):\n","            lst = [create_vector(i, j, row, row[1][i], word, 1) \\\n","                    for j, word in enumerate(new_word_tokenize(row[1][i]))]\n","            ngrams.append([np.array(list(CHAR2VEC.wv[char])) for char in get_character_n_grams(row[1][i], 4)])\n","            sentences.append(lst)\n","        sentences.append(row[1][7] - 1)\n","        development_data.append((sentences, ngrams))\n","        \n","    testing_data = []\n","    for row in test_data.iterrows():\n","        ngrams = []\n","        sentences = [row[1][0]]\n","        for i in range(1, 7):\n","            lst = [create_vector(i, j, row, row[1][i], word, 2) \\\n","                    for j, word in enumerate(new_word_tokenize(row[1][i]))]\n","            ngrams.append([np.array(list(CHAR2VEC.wv[char])) for char in get_character_n_grams(row[1][i], 4)])\n","            sentences.append(lst)\n","        testing_data.append((sentences, ngrams))\n","        \n","    return training_data, development_data, testing_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4l_vtXfJ5FFr","colab_type":"code","colab":{}},"source":["training_data, development_data, testing_data = new_embed(all_data[0], all_data[1], all_data[2])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tz7RqEjSwSfK","colab_type":"code","colab":{}},"source":["class Blind(nn.Module):\n","    def __init__(self):\n","        super(Blind, self).__init__()\n","        self.gru = nn.GRU(EMBED_DIM+17, HIDDEN_DIM, LAYERS, batch_first=True, bidirectional=False)\n","        self.criterion = nn.NLLLoss()\n","        self.cuda(device=DEVICE)\n","\n","    def compute_Loss(self, predicted_vector, gold_label):\n","        return self.criterion(predicted_vector, gold_label)\n","\n","    def forward(self, data):\n","        input_vector = torch.tensor(data, device=DEVICE, dtype=torch.float).unsqueeze(dim=0)\n","        h_0 = torch.zeros((LAYERS, 1, HIDDEN_DIM), device=DEVICE)\n","        output, __ = self.gru(input_vector, h_0)\n","        x = output[0][-1]\n","        return x\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cfipjZIY5FGA","colab_type":"code","colab":{}},"source":["class NSP(nn.Module):\n","    def __init__(self):\n","        super(NSP, self).__init__()\n","        self.beginning = nn.GRU(EMBED_DIM+17, HIDDEN_DIM, LAYERS, batch_first=True, bidirectional=False)\n","        self.ending = nn.GRU(EMBED_DIM+17, HIDDEN_DIM, LAYERS, batch_first=True, bidirectional=False)\n","        self.start_ngram = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidirectional=False)\n","        self.end_ngram = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidirectional=False)\n","        self.blind = Blind()\n","        self.l1 = nn.Linear(HIDDEN_DIM, 2)\n","        self.l2 = nn.Linear(HIDDEN_DIM, 2)\n","        self.l3 = nn.Linear(HIDDEN_DIM, 2)\n","        self.l4 = nn.Linear(25, 2)\n","        # self.linear = nn.Linear(HIDDEN_DIM*1+25, 2)\n","        self.softmax = nn.LogSoftmax(dim=0)\n","        self.criterion = nn.NLLLoss()\n","        self.optimizer = optim.Adam(self.parameters(), lr=LR)\n","        self.cuda(device=DEVICE)\n","        self.testing = False\n","        \n","    def setup(self, data):\n","        input_1 = torch.tensor(np.expand_dims(data[1] + data[2] + data[3] + data[4], axis=0), device=DEVICE, dtype=torch.float)\n","        input_2 = torch.tensor(np.expand_dims(data[5], axis=0), device=DEVICE, dtype=torch.float)\n","        df = all_data[0]\n","        try:\n","            row = df.loc[df['InputStoryid'] == data[0]].values[0]\n","        except: \n","            df = all_data[1]\n","            row = df.loc[df['InputStoryid'] == data[0]].values[0]\n","\n","        sentences = row[1:5]\n","        input_1_sentiment_scores = [get_sentiment_scores(sentence) + [len(sentence.split())] for sentence in sentences]\n","        if self.testing:\n","            correct_column = 5\n","        elif row[7] == 1 and data[6]:\n","            correct_column = 5\n","        elif row[7] == 2 and data[6]:\n","            correct_column = 6\n","        elif row[7] == 1 and not data[6]:\n","            correct_column = 6\n","        else:\n","            correct_column = 5\n","        input_2_sentiment = get_sentiment_scores(row[correct_column]) + [len(row[correct_column].split())]\n","        sentiment = list(np.array(input_1_sentiment_scores).flatten()) + input_2_sentiment \n","        sentiment = torch.tensor(sentiment, device=DEVICE, dtype=torch.float)\n","        return input_1, input_2, sentiment\n","\n","    def compute_Loss(self, predicted_vector, gold_label):\n","        return self.criterion(predicted_vector, gold_label)\n","\n","    def forward(self, data):        \n","        input_1, input_2, sentiment = self.setup(data[0])\n","        input_3 = torch.tensor(np.expand_dims(data[1][0] + data[1][1] + data[1][2] + data[1][3], axis=0), device=DEVICE, dtype=torch.float)\n","        input_4 = torch.tensor(np.expand_dims(data[1][4], axis=0), device=DEVICE, dtype=torch.float)\n","        h_0 = torch.zeros((LAYERS, 1, HIDDEN_DIM), device=DEVICE)\n","        __, h_n = self.beginning(input_1, h_0)\n","        output, __ = self.ending(input_2, h_n)\n","        x = output[0][-1]\n","        x = self.softmax(self.l1(x))\n","        h_0 = torch.zeros((LAYERS, 1, HIDDEN_DIM), device=DEVICE)\n","        __, h_n = self.start_ngram(input_3, h_0)\n","        output, __ = self.end_ngram(input_4, h_n)\n","        y = output[0][-1]\n","        y = self.softmax(self.l2(y))\n","        b = self.blind(data[0][5])\n","        b = self.softmax(self.l3(b))\n","        s = self.softmax(self.l4(sentiment))\n","        # z = torch.cat((y, sentiment), 0)\n","        # z = self.linear(z)\n","        # z = self.softmax(z)\n","        return x + y + b + s"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ST4QTBMq5FGG","colab_type":"code","outputId":"e36c7a9a-3ecd-4e5c-c913-1ce3e6ee6da7","colab":{"base_uri":"https://localhost:8080/","height":884},"executionInfo":{"status":"ok","timestamp":1575936286151,"user_tz":300,"elapsed":823286,"user":{"displayName":"Daniel Parangi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64","userId":"01211561534677666164"}}},"source":["print('Initializing Model')\n","model = NSP()\n","prev_dev_acc = 0.0\n","for epoch in range(EPOCHS):\n","    checkpoint = PATH + '-e' + str((epoch + 1))\n","    model.train()\n","    model.optimizer.zero_grad()\n","    loss = None\n","    correct = 0\n","    total = 0\n","    start_time = time.time()\n","    print('Training started for epoch {}'.format(epoch + 1))\n","    random.shuffle(training_data)\n","    N = len(training_data)\n","    for index  in tqdm(range(N)):\n","        model.optimizer.zero_grad()\n","        input_vector = training_data[index]\n","        gold_label = input_vector[0][6]\n","        predicted_vector = model(input_vector)\n","        predicted_label = torch.argmax(predicted_vector)\n","        correct += int(predicted_label == gold_label)\n","        total += 1\n","        loss = model.compute_Loss(predicted_vector.view(1, -1), torch.tensor([gold_label], device=DEVICE))\n","        loss.backward()\n","        model.optimizer.step()\n","    print('Training accuracy for epoch {}: {}'.format(epoch + 1, correct / total))\n","    correct = 0\n","    total = 0\n","    start_time = time.time()\n","    random.shuffle(development_data)\n","    N = len(development_data)\n","    model.eval()\n","    model.optimizer.zero_grad()\n","    for index in tqdm(range(N)):\n","        sample = development_data[index]\n","        input_1 = (sample[0][0:6] + [sample[0][7]], sample[1][:5])\n","        input_2 = (sample[0][0:5] + sample[0][6:], sample[1][:4] + [sample[1][5]])\n","        gold_label = sample[0][7]\n","        prediction_1 = model(input_1)\n","        prediction_2 = model(input_2)\n","        prob_truthful_1 = prediction_1[1]\n","        prob_false_1 = prediction_1[0]\n","        prob_truthful_2 = prediction_2[1]\n","        prob_false_2 = prediction_2[0]\n","        probs = [prob_truthful_1, prob_false_1, prob_truthful_2, prob_false_2]\n","        max_index = probs.index(max(probs))\n","        if max_index == 0 or max_index == 3:\n","            predicted_label = 0\n","        if max_index == 1 or max_index == 2:\n","            predicted_label = 1\n","        correct += int(predicted_label == gold_label)\n","        total += 1\n","    dev_acc = correct / total\n","    if dev_acc > prev_dev_acc and dev_acc > 0.67:\n","        prev_dev_acc = dev_acc\n","        print('New Best Accuracy: {}'.format(dev_acc))\n","        acc = int(100 * dev_acc)\n","        torch.save(model.state_dict(), checkpoint + '-a' + str(acc) + '.pt')\n","    print('Development accuracy for epoch {}: {}'.format(epoch + 1, correct / total))\n","\n","torch.save(model.state_dict(), PATH + '-final.pt')"],"execution_count":33,"outputs":[{"output_type":"stream","text":["  0%|          | 4/2994 [00:00<01:16, 39.05it/s]"],"name":"stderr"},{"output_type":"stream","text":["Initializing Model\n","Training started for epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 2994/2994 [01:11<00:00, 42.37it/s]\n","  1%|▏         | 5/374 [00:00<00:08, 43.14it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy for epoch 1: 0.5257181028724115\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 374/374 [00:09<00:00, 40.70it/s]\n","  0%|          | 5/2994 [00:00<01:08, 43.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Development accuracy for epoch 1: 0.6149732620320856\n","Training started for epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 2994/2994 [01:13<00:00, 40.73it/s]\n","  1%|          | 4/374 [00:00<00:09, 39.98it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy for epoch 2: 0.5581162324649298\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 374/374 [00:09<00:00, 40.84it/s]\n","  0%|          | 5/2994 [00:00<01:09, 43.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Development accuracy for epoch 2: 0.6390374331550802\n","Training started for epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 2994/2994 [01:12<00:00, 40.13it/s]\n","  1%|▏         | 5/374 [00:00<00:08, 41.23it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy for epoch 3: 0.583500334001336\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 374/374 [00:09<00:00, 41.10it/s]\n","  0%|          | 5/2994 [00:00<01:14, 39.90it/s]"],"name":"stderr"},{"output_type":"stream","text":["Development accuracy for epoch 3: 0.6443850267379679\n","Training started for epoch 4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 2994/2994 [01:12<00:00, 41.29it/s]\n","  1%|          | 4/374 [00:00<00:09, 38.25it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy for epoch 4: 0.5975283901135604\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 374/374 [00:09<00:00, 41.33it/s]\n","  0%|          | 5/2994 [00:00<01:10, 42.56it/s]"],"name":"stderr"},{"output_type":"stream","text":["Development accuracy for epoch 4: 0.660427807486631\n","Training started for epoch 5\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 2994/2994 [01:12<00:00, 41.26it/s]\n","  1%|▏         | 5/374 [00:00<00:08, 44.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy for epoch 5: 0.6068804275217101\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 374/374 [00:09<00:00, 40.90it/s]\n","  0%|          | 5/2994 [00:00<01:05, 45.76it/s]"],"name":"stderr"},{"output_type":"stream","text":["Development accuracy for epoch 5: 0.6417112299465241\n","Training started for epoch 6\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 2994/2994 [01:12<00:00, 42.61it/s]\n","  1%|▏         | 5/374 [00:00<00:08, 42.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy for epoch 6: 0.6269205076820308\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 374/374 [00:09<00:00, 41.30it/s]\n","  0%|          | 5/2994 [00:00<01:08, 43.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Development accuracy for epoch 6: 0.6524064171122995\n","Training started for epoch 7\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 2994/2994 [01:11<00:00, 41.71it/s]\n","  1%|          | 4/374 [00:00<00:09, 37.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy for epoch 7: 0.6295925183700735\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 374/374 [00:09<00:00, 41.33it/s]\n","  0%|          | 5/2994 [00:00<01:07, 44.16it/s]"],"name":"stderr"},{"output_type":"stream","text":["Development accuracy for epoch 7: 0.6497326203208557\n","Training started for epoch 8\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 2994/2994 [01:12<00:00, 41.24it/s]\n","  1%|▏         | 5/374 [00:00<00:08, 41.75it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy for epoch 8: 0.6382765531062125\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 374/374 [00:09<00:00, 39.98it/s]\n","  0%|          | 4/2994 [00:00<01:16, 39.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Development accuracy for epoch 8: 0.6550802139037433\n","Training started for epoch 9\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 2994/2994 [01:15<00:00, 39.70it/s]\n","  1%|▏         | 5/374 [00:00<00:08, 42.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy for epoch 9: 0.6466265865063461\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 374/374 [00:09<00:00, 39.52it/s]\n","  0%|          | 5/2994 [00:00<01:16, 39.18it/s]"],"name":"stderr"},{"output_type":"stream","text":["Development accuracy for epoch 9: 0.6577540106951871\n","Training started for epoch 10\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 2994/2994 [01:14<00:00, 40.14it/s]\n","  1%|▏         | 5/374 [00:00<00:08, 41.90it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy for epoch 10: 0.6519706078824316\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 374/374 [00:09<00:00, 40.73it/s]"],"name":"stderr"},{"output_type":"stream","text":["Development accuracy for epoch 10: 0.6684491978609626\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Sv_VzDxLRtxj","colab_type":"code","colab":{}},"source":["FEATURES = 2\n","# 1: length of last sentence\n","# 2: truthful bigram perplexity of last two sentences\n","# 3: false bigram perplexity of last two sentences\n","# 4: bigram naive bayes truthful/false classification of last two sentences\n","# 5: bigram overlap between last sentence and previous four\n","\n","\n","class LMFeatureExtractor(nn.Module):\n","    def __init__(self):\n","        super(LMFeatureExtractor, self).__init__()\n","        # self.linear = nn.Linear(FEATURES, 2)\n","        # self.softmax = nn.LogSoftmax(dim=0)\n","        # self.cuda(device=DEVICE)\n","        # self.tmodel = TMODEL\n","        # self.fmodel = FMODEL\n","        # self.bnb = BNB\n","\n","    def get_bigram_overlap(self, inputs):\n","        first_four = get_bigram_corpus(inputs[0]+inputs[1]+inputs[2]+inputs[3])\n","        last_one = get_bigram_corpus(inputs[4])\n","        total = 0\n","        intersect = 0\n","        for bigram in last_one:\n","            if bigram in first_four:\n","                intersect += 1\n","            total += 1\n","        overlap = intersect / total\n","        return overlap\n","\n","    def extract_features(self, inputs):\n","        last_two = inputs[3] + inputs[4]\n","        features = []\n","        features.append(len(inputs[4]))\n","        # features.append(self.tmodel.get_perp(last_two))\n","        # features.append(self.fmodel.get_perp(last_two))\n","        # features.append(self.bnb.classify(last_two))\n","        features.append(self.get_bigram_overlap(inputs))\n","        return torch.tensor(features, device=DEVICE, dtype=torch.float)\n","\n","    def forward(self, inputs):\n","        x = self.extract_features(inputs)\n","        # x = self.linear(x)\n","        # return self.softmax(x)\n","        return x\n","\n","\n","class RNNFeatureExtractor(nn.Module):\n","    def __init__(self):\n","        super(RNNFeatureExtractor, self).__init__()\n","        self.lstm = nn.LSTM(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True)\n","        # self.gru = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True)\n","        self.linear = nn.Linear(HIDDEN_DIM, 2)\n","        self.softmax = nn.LogSoftmax(dim=0)\n","        # self.cuda(device=DEVICE)\n","\n","    def forward(self, inputs):\n","        h_0 = torch.zeros(LAYERS, 1, HIDDEN_DIM, device=DEVICE)\n","        # output, __ = self.gru(inputs, h_0)\n","        c_0 = h_0.clone()\n","        output, __ = self.lstm(inputs, (h_0, c_0))\n","        x = output[0][-1]\n","        # x = self.linear(x)\n","        # x = self.softmax(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yf-B7B9IZtlU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}