{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"PartAWorking.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"3qHZm9Eg-rSy","colab_type":"code","outputId":"03af6278-e0ec-483f-e4a7-8a53aeb8fa14","executionInfo":{"status":"ok","timestamp":1575942217821,"user_tz":300,"elapsed":23626,"user":{"displayName":"Bryant Lee","photoUrl":"","userId":"14544020165723410225"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/My Drive/NLP_P4"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n","/gdrive/My Drive/NLP_P4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"24sZTlRs8tUd","colab_type":"code","outputId":"4088e2c7-fe77-4e39-bda9-b7fc056f3a1d","executionInfo":{"status":"ok","timestamp":1575942222626,"user_tz":300,"elapsed":28418,"user":{"displayName":"Bryant Lee","photoUrl":"","userId":"14544020165723410225"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["%pip install vaderSentiment"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting vaderSentiment\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/9e/c53e1fc61aac5ee490a6ac5e21b1ac04e55a7c2aba647bb8411c9aadf24e/vaderSentiment-3.2.1-py2.py3-none-any.whl (125kB)\n","\u001b[K     |████████████████████████████████| 133kB 3.5MB/s \n","\u001b[?25hInstalling collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.2.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"suam4b4K5FEF","colab_type":"code","outputId":"0f6c5cc3-ad4b-4a1a-b43e-4bd9c70ef5b4","executionInfo":{"status":"ok","timestamp":1575942232054,"user_tz":300,"elapsed":8248,"user":{"displayName":"Bryant Lee","photoUrl":"","userId":"14544020165723410225"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["import os\n","import math\n","import time\n","import nltk\n","import random\n","import spacy\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.nn import init\n","from nltk.tokenize import word_tokenize\n","from gensim.models import Word2Vec\n","from collections import Counter\n","from tqdm import tqdm\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","\n","nltk.download('punkt')\n","get_pos = spacy.load(\"en_core_web_sm\")\n","analyser = SentimentIntensityAnalyzer()\n","train_data = pd.read_csv('./gpu_train.csv', encoding='latin-1')\n","dev_data = pd.read_csv('./gpu_dev.csv', encoding='latin-1')\n","test_data = pd.read_csv('./gpu_test.csv', encoding='latin-1')\n","all_data = [train_data, dev_data, test_data]\n","\n","PATH = './'\n","\n","EMBED_DIM = 32\n","HIDDEN_DIM = 16\n","LAYERS = 1\n","EPOCHS = 100\n","LR = 3e-4\n","DEVICE = torch.device(\"cuda:0\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IyQEYCMM5FEX","colab_type":"code","colab":{}},"source":["def get_sentiment_scores(sentence):\n","    '''\n","    [negative, neutral, positive, compound]\n","    '''\n","    return [analyser.polarity_scores(sentence)[sentiment] \\\n","            for sentiment in analyser.polarity_scores(sentence).keys()]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eG_04fPKftzp","colab_type":"code","colab":{}},"source":["def get_character_n_grams(sentence, n):\n","    return [sentence[i:i+n] for i in range(len(sentence)-n+1)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h_GVxlfx5FEx","colab_type":"code","colab":{}},"source":["list_of_sentences = []\n","character_4_grams = []\n","def tag_pos(data):\n","    all_pos = []\n","    pos_counts = Counter()\n","\n","    for df in data:\n","        df_pos = []\n","        for row in df.iterrows():\n","            row_pos = []\n","            for i in range(1, 7):\n","                parts_of_speech = get_pos(row[1][i])\n","                list_of_sentences.append([pos.text for pos in parts_of_speech])\n","                character_4_grams.append(get_character_n_grams(row[1][i], 4))\n","                sentence_pos = [pos.pos_ for pos in parts_of_speech]\n","                row_pos.append(sentence_pos)\n","                pos_counts.update(sentence_pos)\n","            df_pos.append(row_pos)\n","        all_pos.append(df_pos)\n","    return all_pos, pos_counts"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vpJnQpyr5FE7","colab_type":"code","colab":{}},"source":["pos_data = tag_pos(all_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lqeJfOK85FFB","colab_type":"code","outputId":"fa9dec61-9a6a-4811-cdb7-24e679c88c7e","executionInfo":{"status":"ok","timestamp":1575942522007,"user_tz":300,"elapsed":297052,"user":{"displayName":"Bryant Lee","photoUrl":"","userId":"14544020165723410225"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["model = Word2Vec(list_of_sentences, size=EMBED_DIM, min_count=1)\n","name = 'spacy_word2vec' + str(EMBED_DIM) + '.model'\n","model.save(name)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"b56J0A6zg84J","colab_type":"code","outputId":"428265a3-2e02-44ce-df85-ff1cda8917d1","executionInfo":{"status":"ok","timestamp":1575942540211,"user_tz":300,"elapsed":315108,"user":{"displayName":"Bryant Lee","photoUrl":"","userId":"14544020165723410225"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["## n-gram character \n","model = Word2Vec(character_4_grams, size=EMBED_DIM, min_count=1)\n","name = 'n_gram_char_spacy_word2vec' + str(EMBED_DIM) + '.model'\n","model.save(name)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"cl-2JSezAMY7","colab_type":"code","outputId":"7e1ac778-b9cc-43de-a5d5-f3c20a9dd795","executionInfo":{"status":"ok","timestamp":1575942540477,"user_tz":300,"elapsed":315184,"user":{"displayName":"Bryant Lee","photoUrl":"","userId":"14544020165723410225"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["w2v = 'spacy_word2vec' + str(EMBED_DIM) + '.model'\n","WORD2VEC = Word2Vec.load(w2v)\n","c2v = 'n_gram_char_spacy_word2vec' + str(EMBED_DIM) + '.model'\n","CHAR2VEC = Word2Vec.load(c2v)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"nAiqnf9gS8W_","colab_type":"code","colab":{}},"source":["def new_word_tokenize(sentence):\n","    return [pos.text for pos in get_pos(sentence)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_pELXASv5FFP","colab_type":"code","colab":{}},"source":["def get_one_hot(group, row, sentence, word):\n","    pos = pos_data[0][group][row][sentence][word] \n","    return [1 if pos == list(pos_data[1].keys())[i] else 0 for i in range(len(pos_data[1].keys()))]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c9xUUjvZSZ0E","colab_type":"code","colab":{}},"source":["def create_vector(i, j, row, sentence, word, group):\n","    return np.array(list(WORD2VEC.wv[word]) + get_one_hot(group, row[0], i-1, j))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nNe7dxQ6bxll","colab_type":"code","colab":{}},"source":["def new_embed(train_data, dev_data, test_data):\n","    training_data = [] \n","    for row in train_data.iterrows():\n","        pos = [row[1][0]]\n","        neg = [row[1][0]]\n","        pos_n_grams = []\n","        neg_n_grams = []\n","        for i in range(1, 5):\n","            lst = [create_vector(i, j, row, row[1][i], word, 0) \\\n","                    for j, word in enumerate(new_word_tokenize(row[1][i]))]\n","            pos.append(lst)\n","            neg.append(lst)\n","            n_gram_lst = [np.array(list(CHAR2VEC.wv[char])) for char in get_character_n_grams(row[1][i], 4)]\n","            pos_n_grams.append(n_gram_lst)\n","            neg_n_grams.append(n_gram_lst)               \n","        if row[1][7] == 1:\n","            pos.append([np.array(list(WORD2VEC.wv[word]) + get_one_hot(0, row[0], 4, j)) \\\n","                   for j, word in enumerate(new_word_tokenize(row[1][5]))])\n","            neg.append([np.array(list(WORD2VEC.wv[word]) + get_one_hot(0, row[0], 5, j)) \\\n","                   for j, word in enumerate(new_word_tokenize(row[1][6]))])\n","            pos_n_grams.append([np.array(list(CHAR2VEC.wv[char])) for char in get_character_n_grams(row[1][5], 4)])\n","            neg_n_grams.append([np.array(list(CHAR2VEC.wv[char])) for char in get_character_n_grams(row[1][6], 4)])\n","        elif row[1][7] == 2:\n","            pos.append([np.array(list(WORD2VEC.wv[word]) + get_one_hot(0, row[0], 5, j)) \\\n","                   for j, word in enumerate(new_word_tokenize(row[1][6]))])\n","            neg.append([np.array(list(WORD2VEC.wv[word]) + get_one_hot(0, row[0], 4, j)) \\\n","                   for j, word in enumerate(new_word_tokenize(row[1][5]))])\n","            pos_n_grams.append([np.array(list(CHAR2VEC.wv[char])) for char in get_character_n_grams(row[1][6], 4)])\n","            neg_n_grams.append([np.array(list(CHAR2VEC.wv[char])) for char in get_character_n_grams(row[1][5], 4)])\n","        pos.append(1)\n","        neg.append(0)\n","        training_data.append((pos, pos_n_grams))\n","        training_data.append((neg, neg_n_grams))\n","    \n","    development_data = []\n","    for row in dev_data.iterrows():\n","        ngrams = []\n","        sentences = [row[1][0]]\n","        for i in range(1, 7):\n","            lst = [create_vector(i, j, row, row[1][i], word, 1) \\\n","                    for j, word in enumerate(new_word_tokenize(row[1][i]))]\n","            ngrams.append([np.array(list(CHAR2VEC.wv[char])) for char in get_character_n_grams(row[1][i], 4)])\n","            sentences.append(lst)\n","        sentences.append(row[1][7] - 1)\n","        development_data.append((sentences, ngrams))\n","        \n","    testing_data = []\n","    for row in test_data.iterrows():\n","        ngrams = []\n","        sentences = [row[1][0]]\n","        for i in range(1, 7):\n","            lst = [create_vector(i, j, row, row[1][i], word, 2) \\\n","                    for j, word in enumerate(new_word_tokenize(row[1][i]))]\n","            ngrams.append([np.array(list(CHAR2VEC.wv[char])) for char in get_character_n_grams(row[1][i], 4)])\n","            sentences.append(lst)\n","        testing_data.append((sentences, ngrams))\n","        \n","    return training_data, development_data, testing_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4l_vtXfJ5FFr","colab_type":"code","colab":{}},"source":["training_data, development_data, testing_data = new_embed(all_data[0], all_data[1], all_data[2])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tz7RqEjSwSfK","colab_type":"code","colab":{}},"source":["class Blind(nn.Module):\n","    def __init__(self):\n","        super(Blind, self).__init__()\n","        self.gru = nn.GRU(EMBED_DIM+17, HIDDEN_DIM, LAYERS, batch_first=True, bidirectional=False)\n","        self.criterion = nn.NLLLoss()\n","        self.cuda(device=DEVICE)\n","\n","    def compute_Loss(self, predicted_vector, gold_label):\n","        return self.criterion(predicted_vector, gold_label)\n","\n","    def forward(self, data):\n","        input_vector = torch.tensor(data, device=DEVICE, dtype=torch.float).unsqueeze(dim=0)\n","        h_0 = torch.zeros((LAYERS, 1, HIDDEN_DIM), device=DEVICE)\n","        output, __ = self.gru(input_vector, h_0)\n","        x = output[0][-1]\n","        return x\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cfipjZIY5FGA","colab_type":"code","colab":{}},"source":["class NSP(nn.Module):\n","    def __init__(self):\n","        super(NSP, self).__init__()\n","        self.beginning = nn.GRU(EMBED_DIM+17, HIDDEN_DIM, LAYERS, batch_first=True, bidirectional=False)\n","        self.ending = nn.GRU(EMBED_DIM+17, HIDDEN_DIM, LAYERS, batch_first=True, bidirectional=False)\n","        self.start_ngram = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidirectional=False)\n","        self.end_ngram = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidirectional=False)\n","        self.blind = Blind()\n","        self.l1 = nn.Linear(HIDDEN_DIM, 2)\n","        self.l2 = nn.Linear(HIDDEN_DIM, 2)\n","        self.l3 = nn.Linear(HIDDEN_DIM, 2)\n","        self.l4 = nn.Linear(25, 2)\n","        # self.linear = nn.Linear(HIDDEN_DIM*1+25, 2)\n","        self.softmax = nn.LogSoftmax(dim=0)\n","        self.criterion = nn.NLLLoss()\n","        self.optimizer = optim.Adam(self.parameters(), lr=LR)\n","        self.cuda(device=DEVICE)\n","        self.testing = False\n","        \n","    def setup(self, data):\n","        input_1 = torch.tensor(np.expand_dims(data[1] + data[2] + data[3] + data[4], axis=0), device=DEVICE, dtype=torch.float)\n","        input_2 = torch.tensor(np.expand_dims(data[5], axis=0), device=DEVICE, dtype=torch.float)\n","        df = all_data[0]\n","        try:\n","            row = df.loc[df['InputStoryid'] == data[0]].values[0]\n","        except: \n","            df = all_data[1]\n","            row = df.loc[df['InputStoryid'] == data[0]].values[0]\n","\n","        sentences = row[1:5]\n","        input_1_sentiment_scores = [get_sentiment_scores(sentence) + [len(sentence.split())] for sentence in sentences]\n","        if self.testing:\n","            correct_column = 5\n","        elif row[7] == 1 and data[6]:\n","            correct_column = 5\n","        elif row[7] == 2 and data[6]:\n","            correct_column = 6\n","        elif row[7] == 1 and not data[6]:\n","            correct_column = 6\n","        else:\n","            correct_column = 5\n","        input_2_sentiment = get_sentiment_scores(row[correct_column]) + [len(row[correct_column].split())]\n","        sentiment = list(np.array(input_1_sentiment_scores).flatten()) + input_2_sentiment \n","        sentiment = torch.tensor(sentiment, device=DEVICE, dtype=torch.float)\n","        return input_1, input_2, sentiment\n","\n","    def compute_Loss(self, predicted_vector, gold_label):\n","        return self.criterion(predicted_vector, gold_label)\n","\n","    def forward(self, data):        \n","        input_1, input_2, sentiment = self.setup(data[0])\n","        input_3 = torch.tensor(np.expand_dims(data[1][0] + data[1][1] + data[1][2] + data[1][3], axis=0), device=DEVICE, dtype=torch.float)\n","        input_4 = torch.tensor(np.expand_dims(data[1][4], axis=0), device=DEVICE, dtype=torch.float)\n","        h_0 = torch.zeros((LAYERS, 1, HIDDEN_DIM), device=DEVICE)\n","        __, h_n = self.beginning(input_1, h_0)\n","        output, __ = self.ending(input_2, h_n)\n","        x = output[0][-1]\n","        x = self.softmax(self.l1(x))\n","        h_0 = torch.zeros((LAYERS, 1, HIDDEN_DIM), device=DEVICE)\n","        __, h_n = self.start_ngram(input_3, h_0)\n","        output, __ = self.end_ngram(input_4, h_n)\n","        y = output[0][-1]\n","        y = self.softmax(self.l2(y))\n","        b = self.blind(data[0][5])\n","        b = self.softmax(self.l3(b))\n","        s = self.softmax(self.l4(sentiment))\n","        # z = torch.cat((y, sentiment), 0)\n","        # z = self.linear(z)\n","        # z = self.softmax(z)\n","        return x + y + b + s"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ST4QTBMq5FGG","colab_type":"code","outputId":"73c1800b-631d-4c21-a6e6-19407696543b","executionInfo":{"status":"error","timestamp":1575942975929,"user_tz":300,"elapsed":749077,"user":{"displayName":"Bryant Lee","photoUrl":"","userId":"14544020165723410225"}},"colab":{"base_uri":"https://localhost:8080/","height":484}},"source":["print('Initializing Model')\n","model = NSP()\n","prev_dev_acc = 0.0\n","for epoch in range(EPOCHS):\n","    checkpoint = PATH + '-e' + str((epoch + 1))\n","    model.train()\n","    model.optimizer.zero_grad()\n","    loss = None\n","    correct = 0\n","    total = 0\n","    start_time = time.time()\n","    print('Training started for epoch {}'.format(epoch + 1))\n","    random.shuffle(training_data)\n","    N = len(training_data)\n","    for index  in tqdm(range(N)):\n","        model.optimizer.zero_grad()\n","        input_vector = training_data[index]\n","        gold_label = input_vector[0][6]\n","        predicted_vector = model(input_vector)\n","        predicted_label = torch.argmax(predicted_vector)\n","        correct += int(predicted_label == gold_label)\n","        total += 1\n","        loss = model.compute_Loss(predicted_vector.view(1, -1), torch.tensor([gold_label], device=DEVICE))\n","        loss.backward()\n","        model.optimizer.step()\n","    print('Training accuracy for epoch {}: {}'.format(epoch + 1, correct / total))\n","    correct = 0\n","    total = 0\n","    start_time = time.time()\n","    random.shuffle(development_data)\n","    N = len(development_data)\n","    model.eval()\n","    model.optimizer.zero_grad()\n","    for index in tqdm(range(N)):\n","        sample = development_data[index]\n","        input_1 = (sample[0][0:6] + [sample[0][7]], sample[1][:5])\n","        input_2 = (sample[0][0:5] + sample[0][6:], sample[1][:4] + [sample[1][5]])\n","        gold_label = sample[0][7]\n","        prediction_1 = model(input_1)\n","        prediction_2 = model(input_2)\n","        prob_truthful_1 = prediction_1[1]\n","        prob_false_1 = prediction_1[0]\n","        prob_truthful_2 = prediction_2[1]\n","        prob_false_2 = prediction_2[0]\n","        probs = [prob_truthful_1, prob_false_1, prob_truthful_2, prob_false_2]\n","        max_index = probs.index(max(probs))\n","        if max_index == 0 or max_index == 3:\n","            predicted_label = 0\n","        if max_index == 1 or max_index == 2:\n","            predicted_label = 1\n","        correct += int(predicted_label == gold_label)\n","        total += 1\n","    dev_acc = correct / total\n","    if dev_acc > prev_dev_acc and dev_acc > 0.67:\n","        prev_dev_acc = dev_acc\n","        print('New Best Accuracy: {}'.format(dev_acc))\n","        acc = int(100 * dev_acc)\n","        torch.save(model.state_dict(), checkpoint + '-a' + str(acc) + '.pt')\n","    print('Development accuracy for epoch {}: {}'.format(epoch + 1, correct / total))\n","\n","torch.save(model.state_dict(), PATH + '-final.pt')"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Initializing Model\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1/2994 [00:00<06:23,  7.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training started for epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":[" 98%|█████████▊| 2922/2994 [01:58<00:02, 25.71it/s]"],"name":"stderr"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-619442b5a419>\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'InputStoryid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mvalues\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5442\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AXIS_REVERSED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mas_array\u001b[0;34m(self, transpose, items)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_interleave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-d8f9c7696d90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0minput_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mgold_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mpredicted_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mpredicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_label\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgold_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-619442b5a419>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0minput_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0minput_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0minput_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-619442b5a419>\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'InputStoryid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"]}]}]}