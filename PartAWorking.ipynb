{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "get_pos = spacy.load(\"en_core_web_sm\")\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "train_data = pd.read_csv('./train.csv', encoding='latin-1')\n",
    "dev_data = pd.read_csv('./dev.csv', encoding='latin-1')\n",
    "test_data = pd.read_csv('./test.csv', encoding='latin-1')\n",
    "\n",
    "EMBED_DIM = 32\n",
    "HIDDEN_DIM = 16\n",
    "\n",
    "LAYERS = 1\n",
    "EPOCHS = 1000\n",
    "LR = 3e-4\n",
    "\n",
    "data = [train_data, dev_data, test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer_scores(sentence):\n",
    "    return analyser.polarity_scores(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.112, 'pos': 0.888, 'compound': 0.8356}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analyzer_scores('I am super happy!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def replace_sparse_words(train_data, dev_data, test_data):\n",
    "#     train_data = train_data.to_numpy()\n",
    "#     dev_data = dev_data.to_numpy()\n",
    "#     test_data = test_data.to_numpy()\n",
    "#     # Count words\n",
    "\n",
    "#     seen_vocab = {}\n",
    "#     for row in train_data:\n",
    "#         for i in range(1, 7):\n",
    "#             for word in word_tokenize(row[i]):\n",
    "#                 if seen_vocab.get(word) is None:\n",
    "#                     seen_vocab[word] = 1\n",
    "#                 else:\n",
    "#                     seen_vocab[word] += 1\n",
    "#     for row in dev_data:\n",
    "#         for i in range(1, 7):\n",
    "#              for word in word_tokenize(row[i]):\n",
    "#                 if seen_vocab.get(word) is None:\n",
    "#                     seen_vocab[word] = 1\n",
    "#                 else:\n",
    "#                     seen_vocab[word] += 1\n",
    "#     for row in test_data:\n",
    "#         for i in range(1, 7):\n",
    "#              for word in word_tokenize(row[i]):\n",
    "#                 if seen_vocab.get(word) is None:\n",
    "#                     seen_vocab[word] = 1\n",
    "#                 else:\n",
    "#                     seen_vocab[word] += 1\n",
    "\n",
    "#     # Replace words\n",
    "#     new_train = []\n",
    "#     for row in train_data:\n",
    "#         new_pos = [row[0]]\n",
    "#         new_neg = [row[0]]\n",
    "#         for i in range(1, 5):\n",
    "#             new_sentence = []\n",
    "#             for word in word_tokenize(row[i]):\n",
    "#                 new_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n",
    "#             new_pos.append(new_sentence)\n",
    "#             new_neg.append(new_sentence)\n",
    "#         pos_sentence = []\n",
    "#         neg_sentence = []\n",
    "#         if row[7] == 1:\n",
    "#             for word in word_tokenize(row[5]):\n",
    "#                 pos_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n",
    "#             for word in word_tokenize(row[6]):\n",
    "#                 neg_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n",
    "#         else:\n",
    "#             for word in word_tokenize(row[6]):\n",
    "#                 pos_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n",
    "#             for word in word_tokenize(row[5]):\n",
    "#                 neg_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n",
    "#         new_pos.append(pos_sentence)\n",
    "#         new_neg.append(neg_sentence)\n",
    "#         new_pos.append(row[7]-1)\n",
    "#         new_neg.append(row[7]-1)\n",
    "#         new_train.append(new_pos)\n",
    "#         new_train.append(new_neg)\n",
    "\n",
    "#     new_dev = []\n",
    "#     for row in dev_data:\n",
    "#         new_sample = [row[0]]\n",
    "#         for i in range(1, 7):\n",
    "#             new_sentence = []\n",
    "#             for word in word_tokenize(row[i]):\n",
    "#                 new_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n",
    "#             new_sample.append(new_sentence)\n",
    "#         new_sample.append(row[7]-1)\n",
    "#         new_dev.append(new_sample)\n",
    "\n",
    "#     new_test = []\n",
    "#     for row in test_data:\n",
    "#         new_sample = [row[0]]\n",
    "#         for i in range(1, 7):\n",
    "#             new_sentence = []\n",
    "#             for word in word_tokenize(row[i]):\n",
    "#                 new_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n",
    "#             new_sample.append(new_sentence)\n",
    "#         new_test.append(new_sample)\n",
    "\n",
    "#     return pd.DataFrame(new_train), pd.DataFrame(new_dev), pd.DataFrame(new_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = replace_sparse_words(train_data, dev_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentences = []\n",
    "def tag_pos(data):\n",
    "    pos_counts = Counter()\n",
    "    for df in data:\n",
    "        df_pos = []\n",
    "        for row in df.iterrows():\n",
    "            row_pos = []\n",
    "            for i in range(1, 7):\n",
    "                parts_of_speech = get_pos(row[1][i])\n",
    "                list_of_sentences.append(word_tokenize(row[1][i]))\n",
    "                sentence_pos = [pos.pos_ for pos in parts_of_speech]\n",
    "                pos_counts.update(sentence_pos)\n",
    "                row_pos.append(sentence_pos)\n",
    "            df_pos.append(row_pos)\n",
    "    return df_pos, pos_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = tag_pos(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Word2Vec(list_of_sentences, size=EMBED_DIM, min_count=1)\n",
    "# name = 'new_word2vec' + str(EMBED_DIM) + '.model'\n",
    "# model.save(name)\n",
    "\n",
    "w2v = 'new_word2vec' + str(EMBED_DIM) + '.model'\n",
    "WORD2VEC = Word2Vec.load(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(group, row, sentence, word):\n",
    "    print((group, row, sentence, word))\n",
    "    pos = pos_data[group][row][sentence][word]\n",
    "    print(pos)\n",
    "    print()\n",
    "    \n",
    "    return [1 if pos == list(pos_data[1].keys())[i] else 0 for i in range(len(pos_data[1].keys()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(train_data, dev_data, test_data):\n",
    "    training_data = [] \n",
    "    for row in train_data.iterrows():\n",
    "        sentences = [row[1][0]]\n",
    "        for i in range(1, 7):\n",
    "            print(row[1][i])\n",
    "            lst = [np.array(list(WORD2VEC.wv[word]) + get_one_hot(0, row[0], i-1, j)) \\\n",
    "                   for j, word in enumerate(word_tokenize(row[1][i]))]\n",
    "            sentences.append(lst)\n",
    "        sentences.append(row[1][7])\n",
    "        training_data.append(sentences)\n",
    "    \n",
    "    development_data = []\n",
    "    for row in dev_data.iterrows():\n",
    "        sentences = [row[1][0]]\n",
    "        for i in range(1, 7):\n",
    "            lst = [np.array(list(WORD2VEC.wv[word]) + get_one_hot(1, row[0], i-1, j)) \\\n",
    "                   for j, word in enumerate(word_tokenize(row[1][i]))]\n",
    "            sentences.append(lst)\n",
    "        sentences.append(row[1][7])\n",
    "        development_data.append(sentences)\n",
    "        \n",
    "    testing_data = []\n",
    "    for row in test_data.iterrows():\n",
    "        sentences = [row[1][0]]\n",
    "        for i in range(1, 7):\n",
    "            lst = [np.array(list(WORD2VEC.wv[word]) + get_one_hot(2, row[0], i-1, j)) \\\n",
    "                   for j, word in enumerate(word_tokenize(row[1][i]))]\n",
    "            sentences.append(lst)\n",
    "        testing_data.append(sentences)\n",
    "        \n",
    "    return training_data, development_data, testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rick grew up in a troubled household.\n",
      "(0, 0, 0, 0)\n",
      "DET\n",
      "\n",
      "(0, 0, 0, 1)\n",
      "NOUN\n",
      "\n",
      "(0, 0, 0, 2)\n",
      "DET\n",
      "\n",
      "(0, 0, 0, 3)\n",
      "VERB\n",
      "\n",
      "(0, 0, 0, 4)\n",
      "PART\n",
      "\n",
      "(0, 0, 0, 5)\n",
      "VERB\n",
      "\n",
      "(0, 0, 0, 6)\n",
      "ADP\n",
      "\n",
      "(0, 0, 0, 7)\n",
      "DET\n",
      "\n",
      "He never found good support in family, and turned to gangs.\n",
      "(0, 0, 1, 0)\n",
      "PRON\n",
      "\n",
      "(0, 0, 1, 1)\n",
      "VERB\n",
      "\n",
      "(0, 0, 1, 2)\n",
      "PRON\n",
      "\n",
      "(0, 0, 1, 3)\n",
      "AUX\n",
      "\n",
      "(0, 0, 1, 4)\n",
      "DET\n",
      "\n",
      "(0, 0, 1, 5)\n",
      "NOUN\n",
      "\n",
      "(0, 0, 1, 6)\n",
      "ADP\n",
      "\n",
      "(0, 0, 1, 7)\n",
      "NOUN\n",
      "\n",
      "(0, 0, 1, 8)\n",
      "CCONJ\n",
      "\n",
      "(0, 0, 1, 9)\n",
      "ADV\n",
      "\n",
      "(0, 0, 1, 10)\n",
      "VERB\n",
      "\n",
      "(0, 0, 1, 11)\n",
      "PUNCT\n",
      "\n",
      "(0, 0, 1, 12)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-216-905c1fcd8181>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevelopment_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-215-f068fa8f8556>\u001b[0m in \u001b[0;36membed\u001b[0;34m(train_data, dev_data, test_data)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             lst = [np.array(list(WORD2VEC.wv[word]) + get_one_hot(0, row[0], i-1, j)) \\\n\u001b[0;32m----> 8\u001b[0;31m                    for j, word in enumerate(word_tokenize(row[1][i]))]\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-215-f068fa8f8556>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             lst = [np.array(list(WORD2VEC.wv[word]) + get_one_hot(0, row[0], i-1, j)) \\\n\u001b[0;32m----> 8\u001b[0;31m                    for j, word in enumerate(word_tokenize(row[1][i]))]\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-208-0ccdf115cd3a>\u001b[0m in \u001b[0;36mget_one_hot\u001b[0;34m(group, row, sentence, word)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "training_data, development_data, testing_data = embed(data[0], data[1], data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NSP, self).__init__()\n",
    "        self.gru1 = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidrectional=False)\n",
    "        self.gru2 = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidrectional=False)\n",
    "        self.linear = nn.Linear(HIDDEN_DIM+18, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=0)\n",
    "        self.criterion = nn.NLLLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LR)\n",
    "        \n",
    "    def setup(self, data):\n",
    "        input_1 = torch.tensor(np.expand_dims(np.stack(data[1:6], axis=0), axis=0))\n",
    "        input_2 = torch.tensor(np.expand_dims(np.stack((data[1:5] + [data[6]]), axis=0), axis=0))\n",
    "        return input_1, input_2\n",
    "\n",
    "    def compute_Loss(self, predicted_vector, gold_label):\n",
    "        return self.criterion(predicted_vector, gold_label)\n",
    "\n",
    "    def forward(self, data):\n",
    "        input_1, input_2 = self.setup(data)\n",
    "        h_0 = torch.zeros((LAYERS, 1, HIDDEN_DIM), device=DEVICE)\n",
    "        __, h_n = self.gru1(inputs[0], h_0)\n",
    "        output, __ = self.gru2(inputs[1], h_n)\n",
    "        x = output[0][-1]\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print('Initializing Model')\n",
    "    model = NSP()\n",
    "    prev_dev_acc = 0.0\n",
    "    for epoch in range(EPOCHS):\n",
    "        checkpoint = PATH + '-e' + str((epoch + 1))\n",
    "        model.train()\n",
    "        model.optimizer.zero_grad()\n",
    "        loss = None\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        start_time = time.time()\n",
    "        print('Training started for epoch {}'.format(epoch + 1))\n",
    "        random.shuffle(training_data)\n",
    "        N = len(training_data)\n",
    "        for index  in tqdm(range(N)):\n",
    "            model.optimizer.zero_grad()\n",
    "            sample = training_data[index]\n",
    "            input_vector = sample[:7]\n",
    "            gold_label = sample[7]\n",
    "            predicted_vector = model(input_vector)\n",
    "            predicted_label = torch.argmax(predicted_vector)\n",
    "            correct += int(predicted_label == gold_label)\n",
    "            total += 1\n",
    "            loss = model.compute_Loss(predicted_vector.view(1, -1), torch.tensor([gold_label], device=DEVICE))\n",
    "            loss.backward()\n",
    "            model.optimizer.step()\n",
    "        print('Training accuracy for epoch {}: {}'.format(epoch + 1, correct / total))\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        start_time = time.time()\n",
    "        random.shuffle(development_data)\n",
    "        N = len(development_data)\n",
    "        model.eval()\n",
    "        model.optimizer.zero_grad()\n",
    "        for index in tqdm(range(N)):\n",
    "            sample = development_data[index]\n",
    "            input_1 = sample[0:6]\n",
    "            input_2 = sample[0:5] + [sample[6]]\n",
    "            gold_label = sample[7]\n",
    "            prediction_1 = model(input_1)\n",
    "            prediction_2 = model(input_2)\n",
    "            prob_truthful_1 = prediction_1[1]\n",
    "            prob_false_1 = prediction_1[0]\n",
    "            prob_truthful_2 = prediction_2[1]\n",
    "            prob_false_2 = prediction_2[0]\n",
    "            probs = [prob_truthful_1, prob_false_1, prob_truthful_2, prob_false_2]\n",
    "            max_index = probs.index(max(probs))\n",
    "            if max_index == 0 or max_index == 3:\n",
    "                predicted_label = 0\n",
    "            if max_index == 1 or max_index == 2:\n",
    "                predicted_label = 1\n",
    "            correct += int(predicted_label == gold_label)\n",
    "            total += 1\n",
    "        dev_acc = correct / total\n",
    "        if dev_acc > prev_dev_acc and dev_acc > 0.67:\n",
    "            prev_dev_acc = dev_acc\n",
    "            print('New Best Accuracy: {}'.format(dev_acc))\n",
    "            acc = int(100 * dev_acc)\n",
    "            torch.save(model.state_dict(), checkpoint + '-a' + str(acc) + '.pt')\n",
    "        print('Development accuracy for epoch {}: {}'.format(epoch + 1, correct / total))\n",
    "\n",
    "    torch.save(model.state_dict(), PATH + '-final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN\n",
      "VERB\n",
      "ADP\n",
      "ADP\n",
      "DET\n",
      "ADJ\n",
      "NOUN\n",
      "PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = get_pos('Rick grew up in a troubled household.')\n",
    "for token in doc:\n",
    "    print(token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
