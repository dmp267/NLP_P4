{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "get_pos = spacy.load(\"en_core_web_sm\")\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "train_data = pd.read_csv('./train.csv', encoding='latin-1')\n",
    "dev_data = pd.read_csv('./dev.csv', encoding='latin-1')\n",
    "test_data = pd.read_csv('./test.csv', encoding='latin-1')\n",
    "\n",
    "EMBED_DIM = 32\n",
    "HIDDEN_DIM = 16\n",
    "\n",
    "LAYERS = 1\n",
    "EPOCHS = 1000\n",
    "LR = 3e-4\n",
    "\n",
    "data = [train_data, dev_data, test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_scores(sentence):\n",
    "    '''\n",
    "    [negative, neutral, positive, compound]\n",
    "    '''\n",
    "    return [analyser.polarity_scores(sentence)[sentiment] \\\n",
    "            for sentiment in analyser.polarity_scores(sentence).keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = replace_sparse_words(train_data, dev_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentences = []\n",
    "def tag_pos(data):\n",
    "    all_pos = []\n",
    "    pos_counts = Counter()\n",
    "\n",
    "    for df in data:\n",
    "        df_pos = []\n",
    "        for row in df.iterrows():\n",
    "            row_pos = []\n",
    "            for i in range(1, 7):\n",
    "                parts_of_speech = get_pos(row[1][i])\n",
    "                list_of_sentences.append([pos.text for pos in parts_of_speech])\n",
    "                sentence_pos = [pos.pos_ for pos in parts_of_speech]\n",
    "#                 sentence_pos = [row[1][i], sentence_pos] #temporary\n",
    "                row_pos.append(sentence_pos)\n",
    "                pos_counts.update(sentence_pos)\n",
    "            df_pos.append(row_pos)\n",
    "        all_pos.append(df_pos)\n",
    "        \n",
    "#         \n",
    "    return all_pos, pos_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = tag_pos(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('save.txt', 'w') as file:\n",
    "    file.write(str(pos_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Word2Vec(list_of_sentences, size=EMBED_DIM, min_count=1)\n",
    "# name = 'new_word2vec' + str(EMBED_DIM) + '.model'\n",
    "# model.save(name)\n",
    "\n",
    "w2v = 'new_word2vec' + str(EMBED_DIM) + '.model'\n",
    "WORD2VEC = Word2Vec.load(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_data[0][0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(group, row, sentence, word):\n",
    "#     print((group, row, sentence, word))\n",
    "    pos = pos_data[0][group][row][sentence][word]\n",
    "#     pos = pos_data[group][row][sentence][word]\n",
    "#     print(pos)\n",
    "    \n",
    "    return [1 if pos == list(pos_data[1].keys())[i] else 0 for i in range(len(pos_data[1].keys()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_word_tokenize(sentence):\n",
    "    return [pos.text for pos in get_pos(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector(i, j, row, sentence, word, group):\n",
    "    return np.array(list(WORD2VEC.wv[word]) + get_one_hot(group, row[0], i-1, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(train_data, dev_data, test_data):\n",
    "    training_data = [] \n",
    "    for row in train_data.iterrows():\n",
    "        sentences = [row[1][0]]\n",
    "        for i in range(1, 7):\n",
    "#             print(row[1][i])\n",
    "            lst = [create_vector(i, j, row, row[1][i], word, 0) \\\n",
    "                   for j, word in enumerate(new_word_tokenize(row[1][i]))]\n",
    "            sentences.append(lst)\n",
    "        sentences.append(row[1][7])\n",
    "        training_data.append(sentences)\n",
    "#     return training_data, 1, 1\n",
    "    \n",
    "    development_data = []\n",
    "    for row in dev_data.iterrows():\n",
    "        sentences = [row[1][0]]\n",
    "        for i in range(1, 7):\n",
    "            lst = [create_vector(i, j, row, row[1][i], word, 1) \\\n",
    "                   for j, word in enumerate(new_word_tokenize(row[1][i]))]\n",
    "#             lst = [np.array(list(WORD2VEC.wv[word]) + get_one_hot(1, row[0], i-1, j)) \\\n",
    "#                    for j, word in enumerate(new_word_tokenize(row[1][i]))]\n",
    "            sentences.append(lst)\n",
    "        sentences.append(row[1][7])\n",
    "        development_data.append(sentences)\n",
    "        \n",
    "    testing_data = []\n",
    "    for row in test_data.iterrows():\n",
    "        sentences = [row[1][0]]\n",
    "        for i in range(1, 7):\n",
    "            lst = [create_vector(i, j, row, row[1][i], word, 2) \\\n",
    "                   for j, word in enumerate(new_word_tokenize(row[1][i]))]\n",
    "#             lst = [np.array(list(WORD2VEC.wv[word]) + get_one_hot(2, row[0], i-1, j)) \\\n",
    "#                    for j, word in enumerate(new_word_tokenize(row[1][i]))]\n",
    "            sentences.append(lst)\n",
    "        testing_data.append(sentences)\n",
    "        \n",
    "    return training_data, development_data, testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, development_data, testing_data = embed(data[0], data[1], data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('training_embed.txt', 'w') as file:\n",
    "#     file.write(str(training_data))\n",
    "# with open('development_embed.txt', 'w') as file:\n",
    "#     file.write(str(development_data))\n",
    "# with open('testing_embed.txt', 'w') as file:\n",
    "#     file.write(str(testing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rick grew up in a troubled household.\n"
     ]
    }
   ],
   "source": [
    "print(get_pos('Rick grew up in a troubled household.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NSP, self).__init__()\n",
    "        self.gru1 = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidrectional=False)\n",
    "        self.gru2 = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidrectional=False)\n",
    "        self.linear = nn.Linear(HIDDEN_DIM+18+4, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=0)\n",
    "        self.criterion = nn.NLLLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LR)\n",
    "        \n",
    "    def setup(self, data):\n",
    "        input_1 = torch.tensor(np.expand_dims(np.stack(data[1:6], axis=0), axis=0))\n",
    "        input_2 = torch.tensor(np.expand_dims(np.stack((data[1:5] + [data[6]]), axis=0), axis=0))\n",
    "        return input_1, input_2\n",
    "\n",
    "    def compute_Loss(self, predicted_vector, gold_label):\n",
    "        return self.criterion(predicted_vector, gold_label)\n",
    "\n",
    "    def forward(self, data):\n",
    "        input_1, input_2 = self.setup(data)\n",
    "        h_0 = torch.zeros((LAYERS, 1, HIDDEN_DIM), device=DEVICE)\n",
    "        __, h_n = self.gru1(inputs[0], h_0)\n",
    "        output, __ = self.gru2(inputs[1], h_n)\n",
    "        x = output[0][-1]\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print('Initializing Model')\n",
    "    model = NSP()\n",
    "    prev_dev_acc = 0.0\n",
    "    for epoch in range(EPOCHS):\n",
    "        checkpoint = PATH + '-e' + str((epoch + 1))\n",
    "        model.train()\n",
    "        model.optimizer.zero_grad()\n",
    "        loss = None\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        start_time = time.time()\n",
    "        print('Training started for epoch {}'.format(epoch + 1))\n",
    "        random.shuffle(training_data)\n",
    "        N = len(training_data)\n",
    "        for index  in tqdm(range(N)):\n",
    "            model.optimizer.zero_grad()\n",
    "            sample = training_data[index]\n",
    "            input_vector = sample[:7]\n",
    "            gold_label = sample[7]\n",
    "            predicted_vector = model(input_vector)\n",
    "            predicted_label = torch.argmax(predicted_vector)\n",
    "            correct += int(predicted_label == gold_label)\n",
    "            total += 1\n",
    "            loss = model.compute_Loss(predicted_vector.view(1, -1), torch.tensor([gold_label], device=DEVICE))\n",
    "            loss.backward()\n",
    "            model.optimizer.step()\n",
    "        print('Training accuracy for epoch {}: {}'.format(epoch + 1, correct / total))\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        start_time = time.time()\n",
    "        random.shuffle(development_data)\n",
    "        N = len(development_data)\n",
    "        model.eval()\n",
    "        model.optimizer.zero_grad()\n",
    "        for index in tqdm(range(N)):\n",
    "            sample = development_data[index]\n",
    "            input_1 = sample[0:6]\n",
    "            input_2 = sample[0:5] + [sample[6]]\n",
    "            gold_label = sample[7]\n",
    "            prediction_1 = model(input_1)\n",
    "            prediction_2 = model(input_2)\n",
    "            prob_truthful_1 = prediction_1[1]\n",
    "            prob_false_1 = prediction_1[0]\n",
    "            prob_truthful_2 = prediction_2[1]\n",
    "            prob_false_2 = prediction_2[0]\n",
    "            probs = [prob_truthful_1, prob_false_1, prob_truthful_2, prob_false_2]\n",
    "            max_index = probs.index(max(probs))\n",
    "            if max_index == 0 or max_index == 3:\n",
    "                predicted_label = 0\n",
    "            if max_index == 1 or max_index == 2:\n",
    "                predicted_label = 1\n",
    "            correct += int(predicted_label == gold_label)\n",
    "            total += 1\n",
    "        dev_acc = correct / total\n",
    "        if dev_acc > prev_dev_acc and dev_acc > 0.67:\n",
    "            prev_dev_acc = dev_acc\n",
    "            print('New Best Accuracy: {}'.format(dev_acc))\n",
    "            acc = int(100 * dev_acc)\n",
    "            torch.save(model.state_dict(), checkpoint + '-a' + str(acc) + '.pt')\n",
    "        print('Development accuracy for epoch {}: {}'.format(epoch + 1, correct / total))\n",
    "\n",
    "    torch.save(model.state_dict(), PATH + '-final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def replace_sparse_words(train_data, dev_data, test_data):\n",
    "#     train_data = train_data.to_numpy()\n",
    "#     dev_data = dev_data.to_numpy()\n",
    "#     test_data = test_data.to_numpy()\n",
    "#     # Count words\n",
    "\n",
    "#     seen_vocab = {}\n",
    "#     for row in train_data:\n",
    "#         for i in range(1, 7):\n",
    "#             for word in word_tokenize(row[i]):\n",
    "#                 if seen_vocab.get(word) is None:\n",
    "#                     seen_vocab[word] = 1\n",
    "#                 else:\n",
    "#                     seen_vocab[word] += 1\n",
    "#     for row in dev_data:\n",
    "#         for i in range(1, 7):\n",
    "#              for word in word_tokenize(row[i]):\n",
    "#                 if seen_vocab.get(word) is None:\n",
    "#                     seen_vocab[word] = 1\n",
    "#                 else:\n",
    "#                     seen_vocab[word] += 1\n",
    "#     for row in test_data:\n",
    "#         for i in range(1, 7):\n",
    "#              for word in word_tokenize(row[i]):\n",
    "#                 if seen_vocab.get(word) is None:\n",
    "#                     seen_vocab[word] = 1\n",
    "#                 else:\n",
    "#                     seen_vocab[word] += 1\n",
    "\n",
    "#     # Replace words\n",
    "#     new_train = []\n",
    "#     for row in train_data:\n",
    "#         new_pos = [row[0]]\n",
    "#         new_neg = [row[0]]\n",
    "#         for i in range(1, 5):\n",
    "#             new_sentence = []\n",
    "#             for word in word_tokenize(row[i]):\n",
    "#                 new_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n",
    "#             new_pos.append(new_sentence)\n",
    "#             new_neg.append(new_sentence)\n",
    "#         pos_sentence = []\n",
    "#         neg_sentence = []\n",
    "#         if row[7] == 1:\n",
    "#             for word in word_tokenize(row[5]):\n",
    "#                 pos_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n",
    "#             for word in word_tokenize(row[6]):\n",
    "#                 neg_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n",
    "#         else:\n",
    "#             for word in word_tokenize(row[6]):\n",
    "#                 pos_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n",
    "#             for word in word_tokenize(row[5]):\n",
    "#                 neg_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n",
    "#         new_pos.append(pos_sentence)\n",
    "#         new_neg.append(neg_sentence)\n",
    "#         new_pos.append(row[7]-1)\n",
    "#         new_neg.append(row[7]-1)\n",
    "#         new_train.append(new_pos)\n",
    "#         new_train.append(new_neg)\n",
    "\n",
    "#     new_dev = []\n",
    "#     for row in dev_data:\n",
    "#         new_sample = [row[0]]\n",
    "#         for i in range(1, 7):\n",
    "#             new_sentence = []\n",
    "#             for word in word_tokenize(row[i]):\n",
    "#                 new_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n",
    "#             new_sample.append(new_sentence)\n",
    "#         new_sample.append(row[7]-1)\n",
    "#         new_dev.append(new_sample)\n",
    "\n",
    "#     new_test = []\n",
    "#     for row in test_data:\n",
    "#         new_sample = [row[0]]\n",
    "#         for i in range(1, 7):\n",
    "#             new_sentence = []\n",
    "#             for word in word_tokenize(row[i]):\n",
    "#                 new_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n",
    "#             new_sample.append(new_sentence)\n",
    "#         new_test.append(new_sample)\n",
    "\n",
    "#     return pd.DataFrame(new_train), pd.DataFrame(new_dev), pd.DataFrame(new_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
