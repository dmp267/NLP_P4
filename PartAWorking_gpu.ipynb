{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"PartAWorking_gpu.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"3qHZm9Eg-rSy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"f832c96f-ee8f-4d87-d513-fe2ad76a2122","executionInfo":{"status":"ok","timestamp":1575823535052,"user_tz":300,"elapsed":329,"user":{"displayName":"Daniel Parangi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64","userId":"01211561534677666164"}}},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/My Drive/College/F19/CS 4740/NLP_P4"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n","/gdrive/My Drive/College/F19/CS 4740/NLP_P4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"suam4b4K5FEF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"812db820-5d66-4517-f5e0-d094baff2a75","executionInfo":{"status":"ok","timestamp":1575823563068,"user_tz":300,"elapsed":1261,"user":{"displayName":"Daniel Parangi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64","userId":"01211561534677666164"}}},"source":["import os\n","import math\n","import time\n","import nltk\n","import random\n","import spacy\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.nn import init\n","from nltk.tokenize import word_tokenize\n","from gensim.models import Word2Vec\n","from collections import Counter\n","from tqdm import tqdm\n","# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","\n","nltk.download('punkt')\n","get_pos = spacy.load(\"en_core_web_sm\")\n","# analyser = SentimentIntensityAnalyzer()\n","train_data = pd.read_csv('./train.csv', encoding='latin-1')\n","dev_data = pd.read_csv('./dev.csv', encoding='latin-1')\n","test_data = pd.read_csv('./test.csv', encoding='latin-1')\n","data = [train_data, dev_data, test_data]\n","\n","EMBED_DIM = 32\n","HIDDEN_DIM = 16\n","LAYERS = 1\n","EPOCHS = 1000\n","LR = 3e-4\n","DEVICE = torch.device(\"cuda:0\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IyQEYCMM5FEX","colab_type":"code","colab":{}},"source":["# def sentiment_analyzer_scores(sentence):\n","#     return analyser.polarity_scores(sentence)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aTtV2YNG5FEh","colab_type":"code","colab":{}},"source":["# sentiment_analyzer_scores('I am super happy!')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JMazCS_k5FEp","colab_type":"code","colab":{}},"source":["# def replace_sparse_words(train_data, dev_data, test_data):\n","#     train_data = train_data.to_numpy()\n","#     dev_data = dev_data.to_numpy()\n","#     test_data = test_data.to_numpy()\n","#     # Count words\n","\n","#     seen_vocab = {}\n","#     for row in train_data:\n","#         for i in range(1, 7):\n","#             for word in word_tokenize(row[i]):\n","#                 if seen_vocab.get(word) is None:\n","#                     seen_vocab[word] = 1\n","#                 else:\n","#                     seen_vocab[word] += 1\n","#     for row in dev_data:\n","#         for i in range(1, 7):\n","#              for word in word_tokenize(row[i]):\n","#                 if seen_vocab.get(word) is None:\n","#                     seen_vocab[word] = 1\n","#                 else:\n","#                     seen_vocab[word] += 1\n","#     for row in test_data:\n","#         for i in range(1, 7):\n","#              for word in word_tokenize(row[i]):\n","#                 if seen_vocab.get(word) is None:\n","#                     seen_vocab[word] = 1\n","#                 else:\n","#                     seen_vocab[word] += 1\n","\n","#     # Replace words\n","#     new_train = []\n","#     for row in train_data:\n","#         new_pos = [row[0]]\n","#         new_neg = [row[0]]\n","#         for i in range(1, 5):\n","#             new_sentence = []\n","#             for word in word_tokenize(row[i]):\n","#                 new_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n","#             new_pos.append(new_sentence)\n","#             new_neg.append(new_sentence)\n","#         pos_sentence = []\n","#         neg_sentence = []\n","#         if row[7] == 1:\n","#             for word in word_tokenize(row[5]):\n","#                 pos_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n","#             for word in word_tokenize(row[6]):\n","#                 neg_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n","#         else:\n","#             for word in word_tokenize(row[6]):\n","#                 pos_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n","#             for word in word_tokenize(row[5]):\n","#                 neg_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n","#         new_pos.append(pos_sentence)\n","#         new_neg.append(neg_sentence)\n","#         new_pos.append(row[7]-1)\n","#         new_neg.append(row[7]-1)\n","#         new_train.append(new_pos)\n","#         new_train.append(new_neg)\n","\n","#     new_dev = []\n","#     for row in dev_data:\n","#         new_sample = [row[0]]\n","#         for i in range(1, 7):\n","#             new_sentence = []\n","#             for word in word_tokenize(row[i]):\n","#                 new_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n","#             new_sample.append(new_sentence)\n","#         new_sample.append(row[7]-1)\n","#         new_dev.append(new_sample)\n","\n","#     new_test = []\n","#     for row in test_data:\n","#         new_sample = [row[0]]\n","#         for i in range(1, 7):\n","#             new_sentence = []\n","#             for word in word_tokenize(row[i]):\n","#                 new_sentence.append(word if seen_vocab[word] > 3 else '<UNK>')\n","#             new_sample.append(new_sentence)\n","#         new_test.append(new_sample)\n","\n","#     return pd.DataFrame(new_train), pd.DataFrame(new_dev), pd.DataFrame(new_test)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ihxbvB7D5FEs","colab_type":"code","colab":{}},"source":["# data = replace_sparse_words(train_data, dev_data, test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h_GVxlfx5FEx","colab_type":"code","colab":{}},"source":["list_of_sentences = []\n","def tag_pos(data):\n","    pos_counts = Counter()\n","    for df in data:\n","        df_pos = []\n","        for row in df.iterrows():\n","            row_pos = []\n","            for i in range(1, 7):\n","                parts_of_speech = get_pos(row[1][i])\n","                sentence_lst = word_tokenize(row[1][i])\n","                list_of_sentences.append(sentence_lst)\n","                sentence_pos = []\n","                count = 0\n","                iter_pos = iter(parts_of_speech)\n","                for pos in iter_pos:\n","                    if pos.pos_ is 'SPACE':\n","                        continue\n","                    if '-' in sentence_lst[count]:\n","                        sentence_pos.append(pos.pos_)\n","                        for i in range(sentence_lst[count].count('-')):\n","                            next(iter_pos)\n","                            next(iter_pos)\n","                        count += 1\n","                        continue\n","                    sentence_pos.append(pos.pos_)\n","                    count += 1\n","                if not len(sentence_lst) == len(sentence_pos):\n","                    print(sentence_lst)\n","                    print(sentence_pos)\n","                pos_counts.update(sentence_pos)\n","                row_pos.append(sentence_pos)\n","            df_pos.append(row_pos)\n","    return df_pos, pos_counts"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vpJnQpyr5FE7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"6c5213c1-4f1f-484d-c4a8-9a5a2f67431b","executionInfo":{"status":"ok","timestamp":1575827717865,"user_tz":300,"elapsed":30710,"user":{"displayName":"Daniel Parangi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64","userId":"01211561534677666164"}}},"source":["pos_data = tag_pos(data)"],"execution_count":71,"outputs":[{"output_type":"stream","text":["['His', 'colleague-also', 'a', 'chess', 'player-', 'played', 'us', 'as', 'well', '.']\n","['DET', 'NOUN', 'DET', 'NOUN', 'NOUN', 'ADV', 'ADV', 'PUNCT']\n","['Tommy', 'trained', 'hard', 'to', 'run', 'his', 'first', '5K', '.']\n","['PROPN', 'VERB', 'ADV', 'PART', 'VERB', 'DET', 'ADJ', 'NUM']\n","['There', 'were', 'fish', ',', 'sea', 'urchins', ',', 'starfish', '-', 'it', 'was', 'incredible', '!']\n","['ADV', 'VERB', 'NOUN', 'PUNCT', 'NOUN', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'ADJ', 'PUNCT']\n","['He', 'left', 'immediately.', ',']\n","['PRON', 'VERB', 'ADV', 'PUNCT']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lqeJfOK85FFB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"dc60dd02-5b96-4faf-907d-42954e92a271","executionInfo":{"status":"ok","timestamp":1575817032676,"user_tz":300,"elapsed":4453,"user":{"displayName":"Daniel Parangi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64","userId":"01211561534677666164"}}},"source":["# model = Word2Vec(list_of_sentences, size=EMBED_DIM, min_count=1)\n","# name = 'new_word2vec' + str(EMBED_DIM) + '.model'\n","# model.save(name)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"cl-2JSezAMY7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"eb701b24-c995-4364-e7d6-1f8d57b7700e","executionInfo":{"status":"ok","timestamp":1575817032680,"user_tz":300,"elapsed":4434,"user":{"displayName":"Daniel Parangi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64","userId":"01211561534677666164"}}},"source":["w2v = 'new_word2vec' + str(EMBED_DIM) + '.model'\n","WORD2VEC = Word2Vec.load(w2v)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"_pELXASv5FFP","colab_type":"code","colab":{}},"source":["def get_one_hot(group, row, sentence, word):\n","    pos = pos_data[group][row][sentence][word]\n","    return [1 if pos == list(pos_data[1].keys())[i] else 0 for i in range(len(pos_data[1].keys()))]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mc1jQK7B5FFn","colab_type":"code","colab":{}},"source":["def embed(train_data, dev_data, test_data):\n","    training_data = [] \n","    for row in train_data.iterrows():\n","        pos = [row[1][0]]\n","        neg = [row[1][0]]\n","        for i in range(1, 5):\n","            lst = [np.array(list(WORD2VEC.wv[word]) + get_one_hot(0, row[0], i-1, j)) \\\n","                   for j, word in enumerate(word_tokenize(row[1][i]))]\n","            pos.append(lst)\n","            neg.append(lst)\n","        if row[1][7] == 1:\n","            pos.append([np.array(list(WORD2VEC.wv[word]) + get_one_hot(0, row[0], 4, j)) \\\n","                   for j, word in enumerate(word_tokenize(row[1][5]))])\n","            neg.append([np.array(list(WORD2VEC.wv[word]) + get_one_hot(0, row[0], 5, j)) \\\n","                   for j, word in enumerate(word_tokenize(row[1][6]))])\n","        elif row[1][7] == 2:\n","            pos.append([np.array(list(WORD2VEC.wv[word]) + get_one_hot(0, row[0], 5, j)) \\\n","                   for j, word in enumerate(word_tokenize(row[1][6]))])\n","            neg.append([np.array(list(WORD2VEC.wv[word]) + get_one_hot(0, row[0], 4, j)) \\\n","                   for j, word in enumerate(word_tokenize(row[1][5]))])\n","        pos.append(row[1][7])\n","        neg.append(row[1][7])\n","        training_data.append(pos)\n","        training_data.append(neg)\n","    \n","    development_data = []\n","    for row in dev_data.iterrows():\n","        sentences = [row[1][0]]\n","        for i in range(1, 7):\n","            lst = [np.array(list(WORD2VEC.wv[word]) + get_one_hot(1, row[0], i-1, j)) \\\n","                   for j, word in enumerate(word_tokenize(row[1][i]))]\n","            sentences.append(lst)\n","        sentences.append(row[1][7])\n","        development_data.append(sentences)\n","        \n","    testing_data = []\n","    for row in test_data.iterrows():\n","        sentences = [row[1][0]]\n","        for i in range(1, 7):\n","            lst = [np.array(list(WORD2VEC.wv[word]) + get_one_hot(2, row[0], i-1, j)) \\\n","                   for j, word in enumerate(word_tokenize(row[1][i]))]\n","            sentences.append(lst)\n","        testing_data.append(sentences)\n","        \n","    return training_data, development_data, testing_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4l_vtXfJ5FFr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"0d4b888c-8637-4910-c3e6-b337de122bd6","executionInfo":{"status":"error","timestamp":1575817032701,"user_tz":300,"elapsed":4405,"user":{"displayName":"Daniel Parangi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64","userId":"01211561534677666164"}}},"source":["training_data, development_data, testing_data = embed(data[0], data[1], data[2])"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Rick grew up in a troubled household.\n","(0, 0, 0, 0)\n","DET\n","\n","(0, 0, 0, 1)\n","NOUN\n","\n","(0, 0, 0, 2)\n","DET\n","\n","(0, 0, 0, 3)\n","VERB\n","\n","(0, 0, 0, 4)\n","PART\n","\n","(0, 0, 0, 5)\n","VERB\n","\n","(0, 0, 0, 6)\n","ADP\n","\n","(0, 0, 0, 7)\n","DET\n","\n","He never found good support in family, and turned to gangs.\n","(0, 0, 1, 0)\n","PRON\n","\n","(0, 0, 1, 1)\n","VERB\n","\n","(0, 0, 1, 2)\n","PRON\n","\n","(0, 0, 1, 3)\n","VERB\n","\n","(0, 0, 1, 4)\n","DET\n","\n","(0, 0, 1, 5)\n","NOUN\n","\n","(0, 0, 1, 6)\n","ADP\n","\n","(0, 0, 1, 7)\n","NOUN\n","\n","(0, 0, 1, 8)\n","CCONJ\n","\n","(0, 0, 1, 9)\n","ADV\n","\n","(0, 0, 1, 10)\n","VERB\n","\n","(0, 0, 1, 11)\n","PUNCT\n","\n","(0, 0, 1, 12)\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-905c1fcd8181>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevelopment_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-23-a83f696831cf>\u001b[0m in \u001b[0;36membed\u001b[0;34m(train_data, dev_data, test_data)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWORD2VEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mget_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                    \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-23-a83f696831cf>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWORD2VEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mget_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                    \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-0ccdf115cd3a>\u001b[0m in \u001b[0;36mget_one_hot\u001b[0;34m(group, row, sentence, word)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","metadata":{"id":"cfipjZIY5FGA","colab_type":"code","colab":{}},"source":["class NSP(nn.Module):\n","    def __init__(self):\n","        super(NSP, self).__init__()\n","        self.beginning = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidrectional=False)\n","        self.ending = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidrectional=False)\n","        self.linear = nn.Linear(HIDDEN_DIM+18, 2)\n","        self.softmax = nn.LogSoftmax(dim=0)\n","        self.criterion = nn.NLLLoss()\n","        self.optimizer = optim.Adam(self.parameters(), lr=LR)\n","        self.cuda(device=DEVICE)\n","        \n","    def setup(self, data):\n","        input_1 = torch.tensor(np.expand_dims(np.stack(data[:4], axis=0), axis=0), device=DEVICE)\n","        input_2 = torch.tensor(np.expand_dims(np.stack(([data[5]]), axis=0), axis=0), device=DEVICE)\n","        return input_1, input_2\n","\n","    def compute_Loss(self, predicted_vector, gold_label):\n","        return self.criterion(predicted_vector, gold_label)\n","\n","    def forward(self, data):\n","        input_1, input_2 = self.setup(data)\n","        h_0 = torch.zeros((LAYERS, 1, HIDDEN_DIM), device=DEVICE)\n","        __, h_n = self.beginning(input_1, h_0)\n","        output, __ = self.ending(input_2, h_n)\n","        x = output[0][-1]\n","        x = self.linear(x)\n","        x = self.softmax(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ST4QTBMq5FGG","colab_type":"code","colab":{}},"source":["print('Initializing Model')\n","model = NSP()\n","prev_dev_acc = 0.0\n","for epoch in range(EPOCHS):\n","    checkpoint = PATH + '-e' + str((epoch + 1))\n","    model.train()\n","    model.optimizer.zero_grad()\n","    loss = None\n","    correct = 0\n","    total = 0\n","    start_time = time.time()\n","    print('Training started for epoch {}'.format(epoch + 1))\n","    random.shuffle(training_data)\n","    N = len(training_data)\n","    for index  in tqdm(range(N)):\n","        model.optimizer.zero_grad()\n","        sample = training_data[index]\n","        input_vector = sample[1:6]\n","        gold_label = sample[6]\n","        predicted_vector = model(input_vector)\n","        predicted_label = torch.argmax(predicted_vector)\n","        correct += int(predicted_label == gold_label)\n","        total += 1\n","        loss = model.compute_Loss(predicted_vector.view(1, -1), torch.tensor([gold_label], device=DEVICE))\n","        loss.backward()\n","        model.optimizer.step()\n","    print('Training accuracy for epoch {}: {}'.format(epoch + 1, correct / total))\n","    correct = 0\n","    total = 0\n","    start_time = time.time()\n","    random.shuffle(development_data)\n","    N = len(development_data)\n","    model.eval()\n","    model.optimizer.zero_grad()\n","    for index in tqdm(range(N)):\n","        sample = development_data[index]\n","        input_1 = sample[1:6]\n","        input_2 = sample[1:5] + [sample[6]]\n","        gold_label = sample[7]\n","        prediction_1 = model(input_1)\n","        prediction_2 = model(input_2)\n","        prob_truthful_1 = prediction_1[1]\n","        prob_false_1 = prediction_1[0]\n","        prob_truthful_2 = prediction_2[1]\n","        prob_false_2 = prediction_2[0]\n","        probs = [prob_truthful_1, prob_false_1, prob_truthful_2, prob_false_2]\n","        max_index = probs.index(max(probs))\n","        if max_index == 0 or max_index == 3:\n","            predicted_label = 0\n","        if max_index == 1 or max_index == 2:\n","            predicted_label = 1\n","        correct += int(predicted_label == gold_label)\n","        total += 1\n","    dev_acc = correct / total\n","    if dev_acc > prev_dev_acc and dev_acc > 0.67:\n","        prev_dev_acc = dev_acc\n","        print('New Best Accuracy: {}'.format(dev_acc))\n","        acc = int(100 * dev_acc)\n","        torch.save(model.state_dict(), checkpoint + '-a' + str(acc) + '.pt')\n","    print('Development accuracy for epoch {}: {}'.format(epoch + 1, correct / total))\n","\n","torch.save(model.state_dict(), PATH + '-final.pt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j-g2ThNngqq3","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}