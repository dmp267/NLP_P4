{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68735,
     "status": "ok",
     "timestamp": 1575316789300,
     "user": {
      "displayName": "Daniel Parangi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64",
      "userId": "01211561534677666164"
     },
     "user_tz": 300
    },
    "id": "NuXoAYXf8cs9",
    "outputId": "2aef342e-8657-4ea0-ab2e-3579d0c9563f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /gdrive\n",
      "/gdrive/My Drive/College/F19/CS 4740/NLP_P4\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive/My Drive/College/F19/CS 4740/NLP_P4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FSDSEDUXPejS"
   },
   "outputs": [],
   "source": [
    "# %pip install adabound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "el697TJ_9TF0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "from torchsummary import summary\n",
    "# import adabound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LC5r2GaX-bw9"
   },
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 71951,
     "status": "ok",
     "timestamp": 1575316792540,
     "user": {
      "displayName": "Daniel Parangi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64",
      "userId": "01211561534677666164"
     },
     "user_tz": 300
    },
    "id": "TZBYIxzO9UD3",
    "outputId": "c4755cee-e7d3-4e2e-de32-8c39ad6923ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "EMBED_DIM = 32\n",
    "HIDDEN_DIM = 16 # can be 16, 24, 32, 64, 128\n",
    "w2v = 'word2vec' + str(EMBED_DIM) + '.model'\n",
    "WORD2VEC = Word2Vec.load(w2v)\n",
    "NAME = 'JazzDuet'\n",
    "# Full: GRU, No Dropout, EMBED_DIM = 32, HIDDEN_DIM = 16, LAYERS = 1, RMSProp, LR=3e-4, +All Features\n",
    "# Baseline: GRU, No Dropout, EMBED_DIM = 32, HIDDEN_DIM = 16, LAYERS = 1, RMSProp, LR=3e-4, +Lengths\n",
    "\n",
    "LAYERS = 1\n",
    "EPOCHS = 1000\n",
    "LR = 3e-4\n",
    "DEVICE = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9CZmqdQR8meN"
   },
   "outputs": [],
   "source": [
    "CURRENT = os.curdir\n",
    "MODELS = os.path.join(CURRENT, 'experimental_models')\n",
    "PATH = os.path.join(MODELS, NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vdU2TJqu9ULp"
   },
   "outputs": [],
   "source": [
    "def embed(train_data, dev_data, test_data):\n",
    "    train_data = train_data.to_numpy()\n",
    "    dev_data = dev_data.to_numpy()\n",
    "    test_data = test_data.to_numpy()\n",
    "\n",
    "    training_data = []\n",
    "    training_data_prime = [[],[]]\n",
    "    train_omega = []\n",
    "    for row1 in train_data:\n",
    "        sample1 = [[],[]]\n",
    "        pos_lst = []\n",
    "        neg_lst = []\n",
    "        pvec_lst = []\n",
    "        nvec_lst = []\n",
    "        for sentence1 in range(1, 5):\n",
    "            lst = [WORD2VEC.wv[word1] for word1 in row1[sentence1].split()]\n",
    "            if sentence1 < 4:\n",
    "                lst1 = lst\n",
    "                lst2 = [WORD2VEC.wv[word1] for word1 in row1[sentence1+1].split()]\n",
    "                lst1 = np.stack(lst1, axis=0)\n",
    "                lst2 = np.stack(lst2, axis=0)\n",
    "                lst1 = np.expand_dims(lst1, axis=0)\n",
    "                lst2 = np.expand_dims(lst2, axis=0)\n",
    "                train_omega.append(((torch.from_numpy(lst1).to(DEVICE), torch.from_numpy(lst2).to(DEVICE)), 1))\n",
    "            sample1[0] += lst\n",
    "            sample1[1] += lst\n",
    "            pos_lst.append(row1[sentence1].split())\n",
    "            neg_lst.append(row1[sentence1].split())\n",
    "            pvec_lst.append([WORD2VEC.wv[word1] for word1 in row1[sentence1].split()])\n",
    "            nvec_lst.append([WORD2VEC.wv[word1] for word1 in row1[sentence1].split()])\n",
    "        if row1[7] == 1:\n",
    "            sample1[0] += [WORD2VEC.wv[word1] for word1 in row1[5].split()]\n",
    "            sample1[1] += [WORD2VEC.wv[word1] for word1 in row1[6].split()]\n",
    "            pos_lst.append(row1[5].split())\n",
    "            neg_lst.append(row1[6].split())\n",
    "            pos_last = [WORD2VEC.wv[word1] for word1 in row1[5].split()]\n",
    "            neg_last = [WORD2VEC.wv[word1] for word1 in row1[6].split()]\n",
    "            pvec_lst.append([WORD2VEC.wv[word1] for word1 in row1[5].split()])\n",
    "            nvec_lst.append([WORD2VEC.wv[word1] for word1 in row1[6].split()])\n",
    "            pos1 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word1] for word1 in row1[4].split()], axis=0), axis=0)).to(DEVICE)\n",
    "            pos2 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word1] for word1 in row1[5].split()], axis=0), axis=0)).to(DEVICE)\n",
    "            neg1 = pos1\n",
    "            neg2 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word1] for word1 in row1[6].split()], axis=0), axis=0)).to(DEVICE)\n",
    "            train_omega.append(((pos1, pos2), 1))\n",
    "            train_omega.append(((neg1, neg2), 0))\n",
    "        elif row1[7] == 2:\n",
    "            sample1[0] += [WORD2VEC.wv[word1] for word1 in row1[6].split()]\n",
    "            sample1[1] += [WORD2VEC.wv[word1] for word1 in row1[5].split()]\n",
    "            pos_lst.append(row1[6].split())\n",
    "            neg_lst.append(row1[5].split())\n",
    "            pos_last = [WORD2VEC.wv[word1] for word1 in row1[6].split()]\n",
    "            neg_last = [WORD2VEC.wv[word1] for word1 in row1[5].split()]\n",
    "            pvec_lst.append([WORD2VEC.wv[word1] for word1 in row1[6].split()])\n",
    "            nvec_lst.append([WORD2VEC.wv[word1] for word1 in row1[5].split()])\n",
    "            pos1 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word1] for word1 in row1[4].split()], axis=0), axis=0)).to(DEVICE)\n",
    "            pos2 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word1] for word1 in row1[6].split()], axis=0), axis=0)).to(DEVICE)\n",
    "            neg1 = pos1\n",
    "            neg2 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word1] for word1 in row1[5].split()], axis=0), axis=0)).to(DEVICE)\n",
    "            train_omega.append(((pos1, pos2), 1))\n",
    "            train_omega.append(((neg1, neg2), 0))\n",
    "        pos = np.stack(sample1[0], axis=0)\n",
    "        neg = np.stack(sample1[1], axis=0)\n",
    "        pos = np.expand_dims(pos, axis=0)\n",
    "        neg = np.expand_dims(neg, axis=0)\n",
    "        pos_last = np.stack(pos_last, axis=0)\n",
    "        pos_last = np.expand_dims(pos_last, axis=0)\n",
    "        neg_last = np.stack(neg_last, axis=0)\n",
    "        neg_last = np.expand_dims(neg_last, axis=0)\n",
    "        train1 = (torch.from_numpy(pos).to(DEVICE), pos_lst, torch.from_numpy(pos_last).to(DEVICE), pvec_lst)\n",
    "        train2 = (torch.from_numpy(neg).to(DEVICE), neg_lst, torch.from_numpy(neg_last).to(DEVICE), nvec_lst)\n",
    "        # training_data.append((train1, train2))\n",
    "        training_data.append((train1, 1))\n",
    "        training_data.append((train2, 0))\n",
    "        training_data_prime[0].append((torch.from_numpy(pos).to(DEVICE), pos_lst))\n",
    "        training_data_prime[1].append((torch.from_numpy(neg).to(DEVICE), neg_lst))\n",
    "\n",
    "    development_data = []\n",
    "    development_data_prime = [[],[]]\n",
    "    dev_omega = []\n",
    "    for row2 in dev_data:\n",
    "        sample2 = [[],[]]\n",
    "        v1_lst = []\n",
    "        v2_lst = []\n",
    "        vec_lst1 = []\n",
    "        vec_lst2 = []\n",
    "        for sentence2 in range(1, 5):\n",
    "            lst = [WORD2VEC.wv[word2] for word2 in row2[sentence2].split()]\n",
    "            sample2[0] += lst\n",
    "            sample2[1] += lst\n",
    "            v1_lst.append(row2[sentence2].split())\n",
    "            v2_lst.append(row2[sentence2].split())\n",
    "            vec_lst1.append([WORD2VEC.wv[word2] for word2 in row2[sentence2].split()])\n",
    "            vec_lst2.append([WORD2VEC.wv[word2] for word2 in row2[sentence2].split()])\n",
    "        sample2[0] += [WORD2VEC.wv[word2] for word2 in row2[5].split()]\n",
    "        sample2[1] += [WORD2VEC.wv[word2] for word2 in row2[6].split()]\n",
    "        v1_lst.append(row2[5].split())\n",
    "        v2_lst.append(row2[6].split())\n",
    "        v1_last = [WORD2VEC.wv[word2] for word2 in row2[5].split()]\n",
    "        v2_last = [WORD2VEC.wv[word2] for word2 in row2[6].split()]\n",
    "        vec_lst1.append([WORD2VEC.wv[word2] for word2 in row2[5].split()])\n",
    "        vec_lst2.append([WORD2VEC.wv[word2] for word2 in row2[6].split()])\n",
    "        v1 = np.stack(sample2[0], axis=0)\n",
    "        v2 = np.stack(sample2[1], axis=0)\n",
    "        v1 = np.expand_dims(v1, axis=0)\n",
    "        v2 = np.expand_dims(v2, axis=0)\n",
    "        if row2[7] == 1:\n",
    "            pos_lst = v1_lst\n",
    "            neg_lst = v2_lst\n",
    "            pos1 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word2] for word2 in row2[4].split()], axis=0), axis=0)).to(DEVICE)\n",
    "            pos2 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word2] for word2 in row2[5].split()], axis=0), axis=0)).to(DEVICE)\n",
    "            neg1 = pos1\n",
    "            neg2 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word2] for word2 in row2[6].split()], axis=0), axis=0)).to(DEVICE)\n",
    "            pos_pair = (pos1, pos2)\n",
    "            neg_pair = (neg1, neg2)\n",
    "            dev_omega.append((pos_pair, neg_pair, 0))\n",
    "        else:\n",
    "            pos_lst = v2_lst\n",
    "            neg_lst = v1_lst\n",
    "            pos1 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word2] for word2 in row2[4].split()], axis=0), axis=0)).to(DEVICE)\n",
    "            pos2 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word2] for word2 in row2[6].split()], axis=0), axis=0)).to(DEVICE)\n",
    "            neg1 = pos1\n",
    "            neg2 = torch.from_numpy(np.expand_dims(np.stack([WORD2VEC.wv[word2] for word2 in row2[5].split()], axis=0), axis=0)).to(DEVICE)\n",
    "            pos_pair = (pos1, pos2)\n",
    "            neg_pair = (neg1, neg2)\n",
    "            dev_omega.append((pos_pair, neg_pair, 1))\n",
    "        v1_last = np.stack(v1_last, axis=0)\n",
    "        v1_last = np.expand_dims(v1_last, axis=0)\n",
    "        v2_last = np.stack(v2_last, axis=0)\n",
    "        v2_last = np.expand_dims(v2_last, axis=0)\n",
    "        val1 = (torch.from_numpy(v1).to(DEVICE), v1_lst, torch.from_numpy(v1_last).to(DEVICE), vec_lst1)\n",
    "        val2 = (torch.from_numpy(v2).to(DEVICE), v2_lst, torch.from_numpy(v2_last).to(DEVICE), vec_lst2)\n",
    "        development_data.append((val1, val2, row2[7]-1))\n",
    "        development_data_prime[0].append(pos_lst)\n",
    "        development_data_prime[1].append(neg_lst)\n",
    "\n",
    "    testing_data = []\n",
    "    for row3 in test_data:\n",
    "        sample3 = [[],[]]\n",
    "        t1_lst = []\n",
    "        t2_lst = []\n",
    "        for sentence3 in range(1, 5):\n",
    "            lst = [WORD2VEC.wv[word3] for word3 in row3[sentence3].split()]\n",
    "            sample3[0] += lst\n",
    "            sample3[1] += lst\n",
    "            t1_lst.append(row3[sentence3].split())\n",
    "            t2_lst.append(row3[sentence3].split())\n",
    "        sample3[0] += [WORD2VEC.wv[word3] for word3 in row3[5].split()]\n",
    "        sample3[1] += [WORD2VEC.wv[word3] for word3 in row3[6].split()]\n",
    "        t1_lst.append(row3[5].split())\n",
    "        t2_lst.append(row3[6].split())\n",
    "        t1 = np.stack(sample3[0], axis=0)\n",
    "        t2 = np.stack(sample3[1], axis=0)\n",
    "        t1 = np.expand_dims(t1, axis=0)\n",
    "        t2 = np.expand_dims(t2, axis=0)\n",
    "        testing_data.append(((torch.from_numpy(t1).to(DEVICE), t1_lst), (torch.from_numpy(t2).to(DEVICE), t2_lst), row3[0]))\n",
    "\n",
    "    return training_data, development_data, testing_data, training_data_prime, development_data_prime, train_omega, dev_omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_ngrams(data):\n",
    "    sentence_ngrams = {}\n",
    "    for sample in data:\n",
    "        for sentence_index in range(1, 7):\n",
    "            for n in range(1, 6):\n",
    "                ngram_lst = ngrams(sample[sentence_index], n)\n",
    "                for ngram in ngram_lst:\n",
    "                    if sentence_ngrams[ngram] is None:\n",
    "                        sentence_ngrams[ngram] = 1\n",
    "                    else:\n",
    "                        sentence_ngrams[ngram] += 1\n",
    "    for ngram_key in sentence_ngrams.keys():\n",
    "        if sentence_ngrams[ngram_key] < 5:\n",
    "            del sentence_ngrams[ngram_key]\n",
    "    \n",
    "    return sentence_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ALB5RTLzscjy"
   },
   "outputs": [],
   "source": [
    "def replace_sparse_words(train_data, dev_data, test_data):\n",
    "\n",
    "    train_data = train_data.to_numpy()\n",
    "    dev_data = dev_data.to_numpy()\n",
    "    test_data = test_data.to_numpy()\n",
    "    # Count words\n",
    "\n",
    "    seen_vocab = {}\n",
    "    for row in train_data:\n",
    "        for i in range(1, 7):\n",
    "            for word in row[i].split():\n",
    "                if seen_vocab.get(word) is None:\n",
    "                    seen_vocab[word] = 1\n",
    "                else:\n",
    "                    seen_vocab[word] += 1\n",
    "    for row in dev_data:\n",
    "        for i in range(1, 7):\n",
    "             for word in row[i].split():\n",
    "                if seen_vocab.get(word) is None:\n",
    "                    seen_vocab[word] = 1\n",
    "                else:\n",
    "                    seen_vocab[word] += 1\n",
    "    for row in test_data:\n",
    "        for i in range(1, 7):\n",
    "             for word in row[i].split():\n",
    "                if seen_vocab.get(word) is None:\n",
    "                    seen_vocab[word] = 1\n",
    "                else:\n",
    "                    seen_vocab[word] += 1\n",
    "\n",
    "    # Replace words\n",
    "    new_train = []\n",
    "    for row in train_data:\n",
    "        new_sample = []\n",
    "        for i in range(1, 7):\n",
    "            new_sentence = []\n",
    "            for word in row[i].split():\n",
    "                new_sentence.append(word if seen_vocab[word] > 3 else '<UNK>') \n",
    "            new_sample.append(new_sentence)\n",
    "        new_sample.insert(0, row[0])\n",
    "        new_sample.append(row[7])\n",
    "        new_train.append(new_sample)\n",
    "\n",
    "    new_dev = []\n",
    "    for row in dev_data:\n",
    "        new_sample = []\n",
    "        for i in range(1, 7):\n",
    "            new_sentence = []\n",
    "            for word in row[i].split():\n",
    "                new_sentence.append(word if seen_vocab[word] > 3 else '<UNK>') \n",
    "            new_sample.append(new_sentence)\n",
    "        new_sample.insert(0, row[0])\n",
    "        new_sample.append(row[7])\n",
    "        new_dev.append(new_sample)\n",
    "\n",
    "    new_test = []\n",
    "    for row in test_data:\n",
    "        new_sample = []\n",
    "        for i in range(1, 7):\n",
    "            new_sentence = []\n",
    "            for word in row[i].split():\n",
    "                new_sentence.append(word if seen_vocab[word] > 3 else '<UNK>') \n",
    "            new_sample.append(new_sentence)\n",
    "        new_sample.insert(0, row[0])\n",
    "        new_test.append(new_sample)\n",
    "  \n",
    "    return new_train, new_dev, new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zBE6ZiQg9y_d"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./train.csv', encoding='latin-1')\n",
    "dev_data = pd.read_csv('./dev.csv', encoding='latin-1')\n",
    "test_data = pd.read_csv('./test.csv', encoding='latin-1')\n",
    "\n",
    "training_data, development_data, testing_data, training_data_prime, development_data_prime, train_omega, dev_omega = embed(train_data, dev_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aGgSYACWsiMO"
   },
   "outputs": [],
   "source": [
    "new_train, new_dev, new_test = replace_sparse_words(train_data, dev_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ef6bLF1S7nQh"
   },
   "outputs": [],
   "source": [
    "def check_for_unk_words(wordlist, tokenlist):\n",
    "    for i, token in enumerate(wordlist):\n",
    "        if token not in tokenlist:\n",
    "              wordlist[i] = '<UNK>'\n",
    "    return wordlist\n",
    "\n",
    "\n",
    "def get_unigram_corpus(wordlist):\n",
    "    return dict(Counter(wordlist))\n",
    "\n",
    "\n",
    "def get_bigram_corpus(wordlist):\n",
    "    corpus = {}\n",
    "    for i, word in enumerate(wordlist[1:], start=1):\n",
    "        if word != '<s>':\n",
    "            if (wordlist[i-1], word) not in corpus:\n",
    "                corpus[(wordlist[i-1], word)] = 1\n",
    "            else:\n",
    "                corpus[(wordlist[i-1], word)] += 1\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def get_smooth_bigram_corpus(tokenlist, bigram_corpus):\n",
    "    tokenlist.append('<UNK>')\n",
    "    df = pd.DataFrame(1, index = tokenlist, columns = tokenlist) \n",
    "    for bigram in bigram_corpus:\n",
    "        df.loc[bigram[0], bigram[1]] += bigram_corpus[bigram]\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_smooth_bigram_prob(bigram, smooth_bigram_corpus):\n",
    "    return smooth_bigram_corpus.loc[bigram[0], bigram[1]] / smooth_bigram_corpus.loc[bigram[0]].sum()\n",
    "\n",
    "  \n",
    "class NGramModel():\n",
    "    def __init__(self, *args):\n",
    "        super(NGramModel, self).__init__()\n",
    "\n",
    "    def get_perp(self, *args):\n",
    "        return\n",
    "\n",
    "\n",
    "class SmoothBigramModel(NGramModel):\n",
    "    def __init__(self, data):\n",
    "        super(SmoothBigramModel, self).__init__()\n",
    "        data = self.preprocess(data)\n",
    "        self.tokens = list(get_unigram_corpus(data).keys())\n",
    "        corpus = get_bigram_corpus(data)\n",
    "        self.corpus = get_smooth_bigram_corpus(self.tokens, corpus)\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        total = []\n",
    "        for sample in data:\n",
    "            sentences = sample[1]\n",
    "            for sentence in sentences:\n",
    "                total += sentence\n",
    "        return total\n",
    "\n",
    "    def get_perp(self, sentences):\n",
    "        sentences = check_for_unk_words(sentences, self.tokens)\n",
    "        N = len(sentences)\n",
    "        acc = 0\n",
    "        for i, word in enumerate(sentences):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            bigram = (sentences[i-1], word)\n",
    "            acc -= math.log(get_smooth_bigram_prob(bigram, self.corpus))\n",
    "        return math.exp(1/(N-1) * acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CkuKxfdZ-SWV"
   },
   "outputs": [],
   "source": [
    "class bigram_nb_classifier():\n",
    "    def __init__(self, data):\n",
    "        truthful_train = data[0]\n",
    "        false_train = data[1]\n",
    "        truthful_pairs = self.preprocess(truthful_train)\n",
    "        false_pairs = self.preprocess(false_train)\n",
    "        truthful_data = [(self.create_bigram_features(pair), 1) for pair in truthful_pairs]\n",
    "        false_data = [(self.create_bigram_features(pair), 0) for pair in false_pairs]\n",
    "        self.classifier = NaiveBayesClassifier.train(false_data + truthful_data)\n",
    "        self.latest_accuracy = -1\n",
    "\n",
    "    def compute_accuracy(self, val_data):\n",
    "        truthful_val = val_data[0]\n",
    "        false_val = val_data[1]\n",
    "        truthful_pairs = [sentences[3] + sentences[4] for sentences in truthful_val]\n",
    "        false_pairs = [sentences[3] + sentences[4] for sentences in false_val]\n",
    "        truthful_data_v = [(self.create_bigram_features(pair), 1) for pair in truthful_pairs]\n",
    "        false_data_v = [(self.create_bigram_features(pair), 1) for pair in false_pairs]\n",
    "        self.latest_accuracy = nltk.classify.util.accuracy(self.classifier, truthful_data_v + false_data_v )\n",
    "        return self.latest_accuracy\n",
    "    \n",
    "    def create_bigram_features(self, words):\n",
    "        ngram_vocab = ngrams(words, 2)\n",
    "        my_dict = dict([(ng, True) for ng in ngram_vocab])\n",
    "        return my_dict\n",
    "\n",
    "    def preprocess(self, tuple_lst):\n",
    "        story_lst = []\n",
    "        for sample in tuple_lst:\n",
    "            story_lst.append(sample[1])\n",
    "        pair_lst = [sentences[3] + sentences[4] for sentences in story_lst]\n",
    "        return pair_lst\n",
    "    \n",
    "    def classify(self, pair):\n",
    "        return self.classifier.classify(self.create_bigram_features(pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3Gc5cNmsb4w"
   },
   "outputs": [],
   "source": [
    "TMODEL = SmoothBigramModel(training_data_prime[0])\n",
    "FMODEL = SmoothBigramModel(training_data_prime[1])\n",
    "BNB = bigram_nb_classifier(training_data_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nrN77oT2u8mE"
   },
   "outputs": [],
   "source": [
    "# print(BNB.compute_accuracy(development_data_prime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfN6LODr1le7"
   },
   "outputs": [],
   "source": [
    "# data = development_data\n",
    "# random.shuffle(data)\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# N = len(data)\n",
    "# for index in tqdm(range(N)):\n",
    "#     input_1, input_2, gold_label = data[index]\n",
    "#     pair_1 = input_1[1][3] + input_1[1][4]\n",
    "#     pair_2 = input_1[1][3] + input_1[1][4]\n",
    "#     prob_truthful_1 = 1 / TMODEL.get_perp(pair_1)\n",
    "#     prob_false_1 = 1 / FMODEL.get_perp(pair_1)\n",
    "#     prob_truthful_2 = 1 / TMODEL.get_perp(pair_2)\n",
    "#     prob_false_2 = 1 / FMODEL.get_perp(pair_2)\n",
    "#     softmax = nn.Softmax(dim=0)\n",
    "#     probs = torch.tensor([prob_truthful_1, prob_false_1, prob_truthful_2, prob_false_2], device=DEVICE)\n",
    "#     probs = softmax(probs)\n",
    "#     max_index = torch.argmax(probs)\n",
    "#     if max_index == 0 or max_index == 3:\n",
    "#         predicted_label = 0\n",
    "#     if max_index == 1 or max_index == 2:\n",
    "#         predicted_label = 1\n",
    "#     correct += int(predicted_label == gold_label)\n",
    "#     total += 1\n",
    "# print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtBT_1kv8JHf"
   },
   "outputs": [],
   "source": [
    "class MatchDotCOMP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MatchDotCOMP, self).__init__()\n",
    "        # self.lstm = nn.LSTM(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True)\n",
    "        self.gru = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(HIDDEN_DIM*2, 2)\n",
    "        # self.softmax = nn.LogSoftmax(dim=0)\n",
    "        self.criterion = nn.NLLLoss()\n",
    "        self.cuda(device=DEVICE)\n",
    "\n",
    "    def compute_Loss(self, predicted_vector, gold_label):\n",
    "        # print('MatchDotCOMP: predicted = {}, gold = {}'.format(predicted_vector, gold_label))\n",
    "        return self.criterion(predicted_vector, gold_label)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h_0 = torch.zeros((LAYERS*2, 1, HIDDEN_DIM), device=DEVICE)\n",
    "        output, __ = self.gru(inputs, h_0)\n",
    "        # c_0 = h_0.clone()\n",
    "        # output, __ = self.lstm(inputs, (h_0, c_0))\n",
    "        x = output[0][-1]\n",
    "        x = self.linear(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Groot(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Groot, self).__init__()\n",
    "        # self.lstm1 = nn.LSTM(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True)\n",
    "        # self.lstm2 = nn.LSTM(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True)\n",
    "        self.gru1 = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidirectional=True)\n",
    "        self.gru2 = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(HIDDEN_DIM*2, 2)\n",
    "        # self.softmax = nn.LogSoftmax(dim=0)\n",
    "        self.criterion = nn.NLLLoss()\n",
    "        self.cuda(device=DEVICE)\n",
    "\n",
    "    def compute_Loss(self, predicted_vector, gold_label):\n",
    "        # print('Groot: predicted = {}, gold = {}'.format(predicted_vector, gold_label))\n",
    "        return self.criterion(predicted_vector, gold_label)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        input1 = []\n",
    "        for i in range(4):\n",
    "            input1 += inputs[i]\n",
    "        input1 = np.stack(input1, axis=0)\n",
    "        input1 = np.expand_dims(input1, axis=0)\n",
    "        input1 = torch.from_numpy(input1).to(DEVICE)\n",
    "        h_0 = torch.zeros((LAYERS*2, 1, HIDDEN_DIM), device=DEVICE)\n",
    "        # c_0 = h_0.clone()\n",
    "        # __, hcn = self.lstm1(input1, (h_0, c_0))\n",
    "        __, h_n = self.gru1(input1, h_0)\n",
    "        input2 = np.expand_dims(inputs[4], axis=0)\n",
    "        input2 = torch.from_numpy(input2).to(DEVICE)\n",
    "        # h_n = hcn[0]\n",
    "        # c_n = hcn[1]\n",
    "        # output, __ = self.lstm2(input2, (h_n, c_n))\n",
    "        output, __ = self.gru2(input2, h_n)\n",
    "        x = output[0][-1]\n",
    "        x = self.linear(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Blind(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Blind, self).__init__()\n",
    "        # self.lstm = nn.LSTM(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True)\n",
    "        self.gru = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(HIDDEN_DIM*2, 2)\n",
    "        # self.softmax = nn.LogSoftmax(dim=0)\n",
    "        self.criterion = nn.NLLLoss()\n",
    "        self.cuda(device=DEVICE)\n",
    "\n",
    "    def compute_Loss(self, predicted_vector, gold_label):\n",
    "        # print('Blind: predicted = {}, gold = {}'.format(predicted_vector, gold_label))\n",
    "        return self.criterion(predicted_vector, gold_label)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h_0 = torch.zeros((LAYERS*2, 1, HIDDEN_DIM), device=DEVICE)\n",
    "        # c_0 = h_0.clone()\n",
    "        # output, __ = self.lstm(inputs, (h_0, c_0))\n",
    "        output, __ = self.gru(inputs, h_0)\n",
    "        x = output[0][-1]\n",
    "        x = self.linear(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "FEATURES = 5\n",
    "# 1: length of last sentence\n",
    "# 2: truthful bigram perplexity of last two sentences\n",
    "# 3: false bigram perplexity of last two sentences\n",
    "# 4: bigram naive bayes truthful/false classification of last two sentences\n",
    "# 5: bigram overlap between last sentence and previous four\n",
    "\n",
    "\n",
    "class LMFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LMFeatureExtractor, self).__init__()\n",
    "        # self.linear = nn.Linear(FEATURES, 2)\n",
    "        # self.softmax = nn.LogSoftmax(dim=0)\n",
    "        self.cuda(device=DEVICE)\n",
    "        self.tmodel = TMODEL\n",
    "        self.fmodel = FMODEL\n",
    "        self.bnb = BNB\n",
    "\n",
    "    def get_bigram_overlap(self, inputs):\n",
    "        first_four = get_bigram_corpus(inputs[0]+inputs[1]+inputs[2]+inputs[3])\n",
    "        last_one = get_bigram_corpus(inputs[4])\n",
    "        total = 0\n",
    "        intersect = 0\n",
    "        for bigram in last_one:\n",
    "            if bigram in first_four:\n",
    "                intersect += 1\n",
    "            total += 1\n",
    "        overlap = intersect / total\n",
    "        return overlap\n",
    "\n",
    "    def extract_features(self, inputs):\n",
    "        last_two = inputs[3] + inputs[4]\n",
    "        features = []\n",
    "        features.append(len(inputs[4]))\n",
    "        features.append(self.tmodel.get_perp(last_two))\n",
    "        features.append(self.fmodel.get_perp(last_two))\n",
    "        features.append(self.bnb.classify(last_two))\n",
    "        features.append(self.get_bigram_overlap(inputs))\n",
    "        return torch.tensor(features, device=DEVICE, dtype=torch.float)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.extract_features(inputs)\n",
    "        # x = self.linear(x)\n",
    "        # return self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNFeatureExtractor, self).__init__()\n",
    "        # self.lstm = nn.LSTM(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True)\n",
    "        self.gru = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True)\n",
    "        self.linear = nn.Linear(HIDDEN_DIM, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=0)\n",
    "        self.cuda(device=DEVICE)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h_0 = torch.zeros(LAYERS, 1, HIDDEN_DIM, device=DEVICE)\n",
    "        # output, __ = self.gru(inputs, h_0)\n",
    "        c_0 = h_0.clone()\n",
    "        output, __ = self.lstm(inputs, (h_0, c_0))\n",
    "        x = output[0][-1]\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNLM, self).__init__()\n",
    "        # self.rnn_fe = RNNFeatureExtractor()     # P(ending ^ story) ?\n",
    "        self.lm_fe = LMFeatureExtractor()\n",
    "        # self.blind = Blind()                    # P(ending) ?\n",
    "        # self.groot = Groot()                    # P(ending | story) ?\n",
    "        self.linear = nn.Linear(FEATURES, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=0)\n",
    "        self.criterion = nn.NLLLoss()\n",
    "        # self.optimizer = optim.SGD(self.parameters(), lr=LR, momentum=0.9)\n",
    "        # self.optimizer = optim.SGD(self.parameters(), lr=LR, momentum=0.9)\n",
    "        # self.optimizer = optim.Adam(self.parameters(), lr=LR)\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=LR)\n",
    "        # self.optimizer = adabound.AdaBound(self.parameters(), lr=LR, final_lr=0.01)\n",
    "        self.cuda(device=DEVICE)\n",
    "\n",
    "    def compute_Loss(self, predicted_vector, gold_label):\n",
    "        return self.criterion(predicted_vector, gold_label)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # rnn_features = self.rnn_fe(inputs[0])\n",
    "        lm_features = self.lm_fe(inputs[1])\n",
    "        # blind_features = self.blind(inputs[2])\n",
    "        # groot_features = self.groot(inputs[3])\n",
    "        # features = torch.cat((rnn_features, blind_features, groot_features), dim=0)\n",
    "        # features = torch.cat((rnn_features, lm_features, blind_features, groot_features), dim=0)\n",
    "        # features = torch.tensor([math.exp(blind_features[1]), math.exp(groot_features[1]), math.exp(blind_features[1]) / math.exp(groot_features[1])], device=DEVICE)\n",
    "        # features = torch.cat((blind_features, groot_features), dim=0)\n",
    "        x = self.linear(lm_features)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# previously: BigBand\n",
    "# this was dumb\n",
    "class BADBand(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BigBand, self).__init__()\n",
    "        self.p_ending = Blind()\n",
    "        self.p_ending_given_story = Groot()\n",
    "        self.P_ending_and_story = MatchDotCOMP()\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=LR)\n",
    "        self.pv1 = None\n",
    "        self.pv2 = None\n",
    "        self.pv3 = None\n",
    "        self.cuda(device=DEVICE)\n",
    "\n",
    "    def compute_Loss(self, gold_label):\n",
    "        loss1 = self.p_ending.compute_Loss(self.pv1.view(1, -1), gold_label)\n",
    "        loss2 = self.p_ending_given_story.compute_Loss(self.pv2.view(1, -1), gold_label)\n",
    "        loss3 = self.P_ending_and_story.compute_Loss(self.pv3.view(1, -1), gold_label)\n",
    "        return loss1, loss2, loss3\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        pv1 = self.p_ending(inputs[2])\n",
    "        pv2 = self.p_ending_given_story(inputs[3])\n",
    "        pv3 = self.P_ending_and_story(inputs[0])\n",
    "        self.pv1 = pv1\n",
    "        self.pv2 = pv2\n",
    "        self.pv3 = pv3\n",
    "        label1 = torch.argmax(pv1)\n",
    "        label2 = torch.argmax(pv2)\n",
    "        label3 = torch.argmax(pv3)\n",
    "        lst = [label1, label2, label3]\n",
    "        x = max(set(lst), key=lst.count)\n",
    "        return x\n",
    "\n",
    "\n",
    "class JAZZBand(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JAZZBand, self).__init__()\n",
    "        self.saxophone = Blind()\n",
    "        self.trumpet = Groot()\n",
    "        self.drums = MatchDotCOMP()\n",
    "        # self.JAZZ = nn.Linear(HIDDEN_DIM*2, 2)\n",
    "        self.encore = nn.LogSoftmax(dim=0)\n",
    "        self.criterion = nn.NLLLoss()\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=LR)\n",
    "        self.cuda(device=DEVICE)\n",
    "\n",
    "    def compute_Loss(self, predicted_vector, gold_label):\n",
    "        return self.criterion(predicted_vector, gold_label)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        sax = self.saxophone(inputs[2])\n",
    "        trump = self.trumpet(inputs[3])\n",
    "        dr = self.drums(inputs[0])\n",
    "        x = sax + trump + dr\n",
    "        # x = self.JAZZ(x)\n",
    "        x = self.encore(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NSP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NSP, self).__init__()\n",
    "        self.gru1 = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidrectional=True)\n",
    "        self.gru2 = nn.GRU(EMBED_DIM, HIDDEN_DIM, LAYERS, batch_first=True, bidrectional=True)\n",
    "        self.linear = nn.Linear(HIDDEN_DIM*2, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=0)\n",
    "        self.criterion = nn.NLLLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LR)\n",
    "        self.cuda(device=DEVICE)\n",
    "\n",
    "    def compute_Loss(self, predicted_vector, gold_label):\n",
    "        return self.criterion(predicted_vector, gold_label)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h_0 = torch.zeros((LAYERS, 1, HIDDEN_DIM), device=DEVICE)\n",
    "        __, h_n = self.gru1(inputs[0], h_0)\n",
    "        output, __ = self.gru2(inputs[1], h_n)\n",
    "        x = output[0][-1]\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1248437,
     "status": "error",
     "timestamp": 1575319807411,
     "user": {
      "displayName": "Daniel Parangi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBgRNESsNAk7E6CovKP6H8l5JpOEnJ7tIh6JqsL=s64",
      "userId": "01211561534677666164"
     },
     "user_tz": 300
    },
    "id": "YQmeLmMa95Ac",
    "outputId": "a11774fa-9918-4acb-ed13-11ed42d11871"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/2994 [00:00<00:33, 87.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model\n",
      "Training started for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:31<00:00, 94.60it/s]\n",
      "  2%|▏         | 6/374 [00:00<00:06, 53.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 1: 0.1880427521710087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 51.12it/s]\n",
      "  0%|          | 12/2994 [00:00<00:27, 107.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 1: 0.48663101604278075\n",
      "Training started for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:32<00:00, 91.84it/s]\n",
      "  1%|▏         | 5/374 [00:00<00:07, 46.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 2: 0.875751503006012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 48.27it/s]\n",
      "  0%|          | 10/2994 [00:00<00:32, 91.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best Accuracy: 0.6737967914438503\n",
      "Development accuracy for epoch 2: 0.6737967914438503\n",
      "Training started for epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:32<00:00, 91.24it/s]\n",
      "  1%|▏         | 5/374 [00:00<00:08, 45.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 3: 0.938877755511022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 50.65it/s]\n",
      "  0%|          | 11/2994 [00:00<00:27, 109.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 3: 0.5962566844919787\n",
      "Training started for epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:31<00:00, 95.85it/s]\n",
      "  1%|▏         | 5/374 [00:00<00:08, 45.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 4: 0.9515698062792252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 51.80it/s]\n",
      "  0%|          | 10/2994 [00:00<00:31, 95.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 4: 0.5935828877005348\n",
      "Training started for epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:31<00:00, 97.19it/s]\n",
      "  2%|▏         | 6/374 [00:00<00:07, 52.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 5: 0.9512358049432198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 50.76it/s]\n",
      "  0%|          | 10/2994 [00:00<00:31, 95.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 5: 0.6443850267379679\n",
      "Training started for epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:31<00:00, 95.35it/s]\n",
      "  2%|▏         | 7/374 [00:00<00:06, 56.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 6: 0.9498997995991983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 50.64it/s]\n",
      "  0%|          | 8/2994 [00:00<00:42, 70.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 6: 0.6550802139037433\n",
      "Training started for epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:31<00:00, 95.20it/s]\n",
      "  2%|▏         | 7/374 [00:00<00:05, 62.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 7: 0.9509018036072144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 50.75it/s]\n",
      "  0%|          | 11/2994 [00:00<00:28, 105.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 7: 0.5641711229946524\n",
      "Training started for epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:31<00:00, 95.37it/s]\n",
      "  2%|▏         | 6/374 [00:00<00:06, 54.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 8: 0.9555778223112893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 50.05it/s]\n",
      "  0%|          | 11/2994 [00:00<00:28, 106.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 8: 0.6390374331550802\n",
      "Training started for epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:31<00:00, 95.12it/s]\n",
      "  2%|▏         | 6/374 [00:00<00:06, 55.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 9: 0.9539078156312625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 51.00it/s]\n",
      "  0%|          | 12/2994 [00:00<00:26, 112.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 9: 0.6283422459893048\n",
      "Training started for epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:31<00:00, 95.47it/s]\n",
      "  2%|▏         | 6/374 [00:00<00:06, 55.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 10: 0.9555778223112893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 50.73it/s]\n",
      "  0%|          | 10/2994 [00:00<00:30, 97.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 10: 0.6336898395721925\n",
      "Training started for epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:31<00:00, 94.12it/s]\n",
      "  1%|▏         | 5/374 [00:00<00:07, 46.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 11: 0.9519038076152304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 50.92it/s]\n",
      "  0%|          | 12/2994 [00:00<00:28, 106.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 11: 0.6016042780748663\n",
      "Training started for epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:31<00:00, 96.45it/s]\n",
      "  1%|▏         | 5/374 [00:00<00:08, 43.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 12: 0.9575818303273214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 50.87it/s]\n",
      "  0%|          | 9/2994 [00:00<00:35, 83.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 12: 0.6283422459893048\n",
      "Training started for epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:30<00:00, 96.72it/s]\n",
      "  2%|▏         | 6/374 [00:00<00:07, 51.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 13: 0.9542418169672678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 52.16it/s]\n",
      "  0%|          | 11/2994 [00:00<00:28, 104.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 13: 0.6310160427807486\n",
      "Training started for epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:30<00:00, 97.27it/s]\n",
      "  1%|▏         | 5/374 [00:00<00:08, 42.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 14: 0.9565798263193053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 50.22it/s]\n",
      "  0%|          | 12/2994 [00:00<00:26, 114.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 14: 0.6096256684491979\n",
      "Training started for epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:30<00:00, 96.63it/s] \n",
      "  2%|▏         | 6/374 [00:00<00:06, 58.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 15: 0.9542418169672678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 51.70it/s]\n",
      "  0%|          | 10/2994 [00:00<00:31, 93.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 15: 0.5909090909090909\n",
      "Training started for epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:30<00:00, 96.76it/s]\n",
      "  1%|▏         | 5/374 [00:00<00:08, 46.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 16: 0.9605878423513694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 52.21it/s]\n",
      "  0%|          | 9/2994 [00:00<00:33, 88.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 16: 0.6310160427807486\n",
      "Training started for epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:30<00:00, 96.63it/s]\n",
      "  2%|▏         | 6/374 [00:00<00:07, 49.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 17: 0.9545758183032732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 52.17it/s]\n",
      "  0%|          | 11/2994 [00:00<00:29, 100.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 17: 0.5802139037433155\n",
      "Training started for epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:30<00:00, 97.33it/s]\n",
      "  2%|▏         | 6/374 [00:00<00:06, 53.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 18: 0.9589178356713427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 51.66it/s]\n",
      "  0%|          | 11/2994 [00:00<00:29, 100.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 18: 0.6283422459893048\n",
      "Training started for epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:31<00:00, 94.61it/s]\n",
      "  2%|▏         | 6/374 [00:00<00:06, 58.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 19: 0.957247828991316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 50.97it/s]\n",
      "  0%|          | 9/2994 [00:00<00:33, 88.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 19: 0.5775401069518716\n",
      "Training started for epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:30<00:00, 97.39it/s]\n",
      "  1%|          | 4/374 [00:00<00:09, 38.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 20: 0.9605878423513694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 52.54it/s]\n",
      "  0%|          | 10/2994 [00:00<00:31, 96.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 20: 0.6336898395721925\n",
      "Training started for epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:30<00:00, 97.12it/s]\n",
      "  2%|▏         | 6/374 [00:00<00:06, 58.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 21: 0.9585838343353373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 51.01it/s]\n",
      "  0%|          | 11/2994 [00:00<00:27, 106.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 21: 0.6310160427807486\n",
      "Training started for epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:30<00:00, 97.10it/s]\n",
      "  1%|▏         | 5/374 [00:00<00:09, 40.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 22: 0.9595858383433534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 51.87it/s]\n",
      "  0%|          | 9/2994 [00:00<00:36, 81.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 22: 0.5935828877005348\n",
      "Training started for epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:30<00:00, 97.13it/s] \n",
      "  2%|▏         | 7/374 [00:00<00:05, 63.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 23: 0.9559118236472945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 50.24it/s]\n",
      "  0%|          | 10/2994 [00:00<00:32, 91.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 23: 0.6283422459893048\n",
      "Training started for epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:30<00:00, 97.09it/s] \n",
      "  1%|▏         | 5/374 [00:00<00:07, 49.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 24: 0.9639278557114228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 52.42it/s]\n",
      "  0%|          | 10/2994 [00:00<00:29, 99.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 24: 0.6176470588235294\n",
      "Training started for epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:30<00:00, 97.60it/s]\n",
      "  2%|▏         | 7/374 [00:00<00:06, 60.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 25: 0.958249832999332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 51.36it/s]\n",
      "  0%|          | 12/2994 [00:00<00:26, 111.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 25: 0.5935828877005348\n",
      "Training started for epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:30<00:00, 97.13it/s]\n",
      "  2%|▏         | 6/374 [00:00<00:06, 53.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 26: 0.958249832999332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 51.99it/s]\n",
      "  0%|          | 11/2994 [00:00<00:31, 96.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 26: 0.6096256684491979\n",
      "Training started for epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:30<00:00, 97.05it/s]\n",
      "  1%|▏         | 5/374 [00:00<00:08, 43.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 27: 0.9559118236472945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 50.04it/s]\n",
      "  0%|          | 10/2994 [00:00<00:30, 98.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 27: 0.5909090909090909\n",
      "Training started for epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:30<00:00, 96.82it/s]\n",
      "  1%|▏         | 5/374 [00:00<00:08, 45.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 28: 0.9599198396793587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 52.19it/s]\n",
      "  0%|          | 11/2994 [00:00<00:27, 107.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 28: 0.6176470588235294\n",
      "Training started for epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:30<00:00, 97.38it/s]\n",
      "  2%|▏         | 6/374 [00:00<00:07, 50.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 29: 0.9555778223112893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 51.35it/s]\n",
      "  0%|          | 10/2994 [00:00<00:31, 95.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 29: 0.5935828877005348\n",
      "Training started for epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:30<00:00, 96.83it/s]\n",
      "  2%|▏         | 6/374 [00:00<00:06, 56.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 30: 0.9592518370073481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 53.79it/s]\n",
      "  0%|          | 10/2994 [00:00<00:30, 97.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 30: 0.6149732620320856\n",
      "Training started for epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:30<00:00, 97.56it/s] \n",
      "  1%|▏         | 5/374 [00:00<00:08, 44.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 31: 0.9592518370073481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 51.87it/s]\n",
      "  0%|          | 9/2994 [00:00<00:34, 87.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 31: 0.6176470588235294\n",
      "Training started for epoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2994/2994 [00:31<00:00, 96.49it/s]\n",
      "  1%|▏         | 5/374 [00:00<00:08, 42.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for epoch 32: 0.9609218436873748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:07<00:00, 51.59it/s]\n",
      "  0%|          | 9/2994 [00:00<00:33, 89.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development accuracy for epoch 32: 0.6203208556149733\n",
      "Training started for epoch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 1680/2994 [00:17<00:13, 97.28it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-ebb87d18f1b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0minput_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# input_vector, gold_label = train_omega[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mpredicted_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mpredicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# predicted_label = model(input_vector)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-23aeb5a711d3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# rnn_features = self.rnn_fe(inputs[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mlm_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_fe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;31m# blind_features = self.blind(inputs[2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# groot_features = self.groot(inputs[3])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-23aeb5a711d3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;31m# x = self.linear(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# return self.softmax(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-23aeb5a711d3>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_perp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_perp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-e546e3e9e936>\u001b[0m in \u001b[0;36mget_perp\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mbigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0macc\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_smooth_bigram_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-e546e3e9e936>\u001b[0m in \u001b[0;36mget_smooth_bigram_prob\u001b[0;34m(bigram, smooth_bigram_corpus)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_smooth_bigram_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_bigram_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msmooth_bigram_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msmooth_bigram_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mstat_func\u001b[0;34m(self, axis, skipna, level, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m  11583\u001b[0m             \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11584\u001b[0m             \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11585\u001b[0;31m             \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11586\u001b[0m         )\n\u001b[1;32m  11587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   4088\u001b[0m                 )\n\u001b[1;32m   4089\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4090\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4092\u001b[0m         \u001b[0;31m# TODO(EA) dispatch to Index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mobj_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"reduction operation {name!r} not allowed for this dtype\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nan\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mobj_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"reduction operation {name!r} not allowed for this dtype\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nan\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Initializing Model')\n",
    "model = RNNLM()\n",
    "prev_dev_acc = 0.0\n",
    "for epoch in range(EPOCHS):\n",
    "    checkpoint = PATH + '-e' + str((epoch + 1))\n",
    "    model.train()\n",
    "    model.optimizer.zero_grad()\n",
    "    loss = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    print('Training started for epoch {}'.format(epoch + 1))\n",
    "    random.shuffle(training_data)\n",
    "    # random.shuffle(train_omega)\n",
    "    N = len(training_data)\n",
    "    # N = len(train_omega)\n",
    "    for index  in tqdm(range(N)):\n",
    "        model.optimizer.zero_grad()\n",
    "        input_vector, gold_label = training_data[index]\n",
    "        # input_vector, gold_label = train_omega[index]\n",
    "        predicted_vector = model(input_vector)\n",
    "        predicted_label = torch.argmax(predicted_vector)\n",
    "        correct += int(predicted_label == gold_label)\n",
    "        total += 1\n",
    "        loss = model.compute_Loss(predicted_vector.view(1, -1), torch.tensor([gold_label], device=DEVICE))\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "    print('Training accuracy for epoch {}: {}'.format(epoch + 1, correct / total))\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    random.shuffle(development_data)\n",
    "    # random.shuffle(dev_omega)\n",
    "    N = len(development_data)\n",
    "    # N = len(dev_omega)\n",
    "    model.eval()\n",
    "    model.optimizer.zero_grad()\n",
    "    for index in tqdm(range(N)):\n",
    "        input_1, input_2, gold_label = development_data[index]\n",
    "        # input_1, input_2, gold_label = dev_omega[index]\n",
    "        prediction_1 = model(input_1)\n",
    "        prediction_2 = model(input_2)\n",
    "        prob_truthful_1 = prediction_1[1]\n",
    "        prob_false_1 = prediction_1[0]\n",
    "        prob_truthful_2 = prediction_2[1]\n",
    "        prob_false_2 = prediction_2[0]\n",
    "        probs = [prob_truthful_1, prob_false_1, prob_truthful_2, prob_false_2]\n",
    "        max_index = probs.index(max(probs))\n",
    "        if max_index == 0 or max_index == 3:\n",
    "            predicted_label = 0\n",
    "        if max_index == 1 or max_index == 2:\n",
    "            predicted_label = 1\n",
    "        correct += int(predicted_label == gold_label)\n",
    "        total += 1\n",
    "    dev_acc = correct / total\n",
    "    if dev_acc > prev_dev_acc and dev_acc > 0.67:\n",
    "        prev_dev_acc = dev_acc\n",
    "        print('New Best Accuracy: {}'.format(dev_acc))\n",
    "        acc = int(100 * dev_acc)\n",
    "        torch.save(model.state_dict(), checkpoint + '-a' + str(acc) + '.pt')\n",
    "    print('Development accuracy for epoch {}: {}'.format(epoch + 1, correct / total))\n",
    "\n",
    "torch.save(model.state_dict(), PATH + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SVtFlIjDJg09"
   },
   "outputs": [],
   "source": [
    "# Test : Kaggle A\n",
    "# model = RNNLM()\n",
    "# model.load_state_dict(torch.load(os.path.join(MODELS, 'rnnlm-baseline-e10-a67.pt')))\n",
    "\n",
    "# N = len(testing_data)\n",
    "# ids = []\n",
    "# predictions = []\n",
    "# for index in tqdm(range(N)):\n",
    "#     input_1, input_2, id_tag = testing_data[index]\n",
    "#     prediction_1 = model(input_1)\n",
    "#     prediction_2 = model(input_2)\n",
    "\n",
    "#     prob_truthful_1 = prediction_1[1]\n",
    "#     prob_false_1 = prediction_1[0]\n",
    "#     prob_truthful_2 = prediction_2[1]\n",
    "#     prob_false_2 = prediction_2[0]\n",
    "\n",
    "#     probs = [prob_truthful_1, prob_false_1, prob_truthful_2, prob_false_2]\n",
    "\n",
    "#     max_index = probs.index(max(probs))\n",
    "#     if max_index == 0 or max_index == 3:\n",
    "#         predicted_label = 0\n",
    "#     if max_index == 1 or max_index == 2:\n",
    "#         predicted_label = 1\n",
    "#     ids.append(id_tag)\n",
    "#     predictions.append(predicted_label + 1)\n",
    "\n",
    "# df = pd.DataFrame({'Id': ids, 'Prediction': predictions}, columns = ['Id', 'Prediction'])\n",
    "# df.to_csv('Part-A_rnnlm-baseline-e10-a67.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
